{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"/home/brysongray/tractography\")\n",
    "import load_data\n",
    "from image import Image\n",
    "import make_simulated_neurons as msn\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "dtype = torch.float32\n",
    "\n",
    "date = datetime.now().strftime(\"%m-%d-%y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv3d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm3d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv3d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm3d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.infilters = 64\n",
    "        # self.conv1 = nn.Sequential(\n",
    "        #                 nn.Conv3d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "        #                 nn.BatchNorm3d(64),\n",
    "        #                 nn.ReLU())\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv3d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm3d(64),\n",
    "                        nn.ReLU())\n",
    "        # self.maxpool = nn.MaxPool3d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 1)\n",
    "        # self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avgpool = nn.AvgPool3d(4, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, filters, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.infilters != filters:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.infilters, filters, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm3d(filters),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.infilters, filters, stride, downsample))\n",
    "        self.infilters = filters\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.infilters, filters))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "def collect_data(labels_dir, image_dir, out_dir, name, date, samples_per_file=500, rng=None):\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    datasets = [dir for dir in os.listdir(labels_dir) if dir[0] != \".\"]\n",
    "    files = []\n",
    "    for dataset in datasets:\n",
    "        labels_path = os.path.join(labels_dir, dataset)\n",
    "        if \"CNG version\" in os.listdir(labels_path):\n",
    "            labels_path = labels_path + \"/CNG version\"\n",
    "        elif \"Source-Version\" in os.listdir(labels_path):\n",
    "            labels_path = labels_path + \"/Source-Version\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for f in os.listdir(labels_path):\n",
    "            if f.endswith(\"swc\") and f[0] != \".\":\n",
    "                files.append(os.path.join(labels_path, f))\n",
    "\n",
    "    sample_points = {}\n",
    "    for labels_file in files:\n",
    "        swc_list = load_data.read_swc(labels_file)\n",
    "        sections, section_graph, branches, terminals, scale = load_data.parse_swc_list(swc_list, adjust=True)\n",
    "        rand_sections = rng.choice(list(sections.keys()), size=samples_per_file)\n",
    "        points = []\n",
    "        for i in rand_sections:\n",
    "            section_flat = sections[i].flatten(0,1) # type: ignore # \n",
    "            random_point = rng.choice(np.arange(len(section_flat)))\n",
    "            random_point = section_flat[random_point]\n",
    "            # random translation vector from normal distribution about random_point\n",
    "            translation = rng.uniform(low=0.0, high=1.0, size=(3,))*8.0 - 4.0\n",
    "            random_point += translation\n",
    "            points.append(random_point)\n",
    "        points = np.array(points)\n",
    "        fname = labels_file.split(\"/\")[-1].split(\".CNG\")[0]\n",
    "        sample_points[fname] = points\n",
    "\n",
    "    os.makedirs(os.path.join(out_dir,\"observations\"), exist_ok=True)\n",
    "    image_files = os.listdir(image_dir)\n",
    "    annotations = {}\n",
    "    obs_id = 0\n",
    "    for f in image_files:\n",
    "        fname = f.split(\".CNG\")[0]\n",
    "        points = sample_points[fname]\n",
    "        data = torch.load(os.path.join(image_dir, f), weights_only=True)\n",
    "        img = data[\"image\"]\n",
    "        img = Image(img)\n",
    "        branch_mask = data[\"branch_mask\"]\n",
    "        for point in points:\n",
    "            patch, _ = img.crop(torch.tensor(point), 7, pad=True, value=0.0)\n",
    "            i,j,k = [int(np.round(x)) for x in point]\n",
    "            label = branch_mask[0, i, j, k].item()\n",
    "            fname = f\"obs_{obs_id}.pt\"\n",
    "            torch.save(patch, os.path.join(os.path.join(out_dir, \"observations\"), fname))\n",
    "            annotations[fname] = label\n",
    "            obs_id += 1\n",
    "\n",
    "    # save annotations\n",
    "    df = pd.DataFrame.from_dict(annotations, orient=\"index\")\n",
    "    df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_annotations.csv\"), index=False)\n",
    "    # split into test and training data\n",
    "    data_permutation = torch.randperm(len(annotations))\n",
    "    test_idxs = data_permutation[:len(data_permutation)//5].tolist()\n",
    "    training_idxs = data_permutation[len(data_permutation)//5:].tolist()\n",
    "    training_annotations = {list(annotations)[i]: list(annotations.values())[i] for i in training_idxs}\n",
    "    test_annotations = {list(annotations)[i]: list(annotations.values())[i] for i in test_idxs}\n",
    "    # save \n",
    "    df = pd.DataFrame.from_dict(training_annotations, orient=\"index\")\n",
    "    df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_training_annotations.csv\"))\n",
    "    df = pd.DataFrame.from_dict(test_annotations, orient=\"index\")\n",
    "    df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_test_annotations.csv\"))\n",
    "\n",
    "\n",
    "def swc_random_points(samples_per_neuron, swc_lists, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    sample_points = {}\n",
    "    for i, swc_list in enumerate(swc_lists):\n",
    "        sections, section_graph, branches, terminals, scale = load_data.parse_swc_list(swc_list, adjust=False)\n",
    "        rand_sections = rng.choice(list(sections.keys()), size=samples_per_neuron)\n",
    "        points = []\n",
    "        for j in rand_sections:\n",
    "            section_flat = sections[j].flatten(0,1) # type: ignore # \n",
    "            random_point = rng.choice(np.arange(len(section_flat)))\n",
    "            random_point = section_flat[random_point]\n",
    "            # random translation vector from normal distribution about random_point\n",
    "            translation = rng.uniform(low=0.0, high=1.0, size=(3,))*8.0 - 4.0\n",
    "            random_point += translation\n",
    "            points.append(random_point)\n",
    "        points = np.array(points)\n",
    "        \n",
    "        sample_points[i] = points\n",
    "    \n",
    "    return sample_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 707/1000 [00:03<00:01, 231.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m swc_lists \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(count)):\n\u001b[0;32m----> 9\u001b[0m     swc_list \u001b[38;5;241m=\u001b[39m \u001b[43mmsn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_swc_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[43muniform_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mrandom_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_branches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# make simulated neuron paths.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     swc_lists\u001b[38;5;241m.\u001b[39mappend(swc_list)\n\u001b[1;32m     19\u001b[0m sample_points \u001b[38;5;241m=\u001b[39m swc_random_points(\u001b[38;5;241m50\u001b[39m, swc_lists)\n",
      "File \u001b[0;32m~/tractography/data/make_simulated_neurons.py:161\u001b[0m, in \u001b[0;36mmake_swc_list\u001b[0;34m(size, length, step_size, kappa, uniform_len, random_start, rng, num_branches)\u001b[0m\n\u001b[1;32m    159\u001b[0m branch_points\u001b[38;5;241m.\u001b[39mappend(branch_start)\n\u001b[1;32m    160\u001b[0m branch_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(t)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m branch_start)\n\u001b[0;32m--> 161\u001b[0m new_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbranch_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniform_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muniform_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m graph\u001b[38;5;241m.\u001b[39mappend([graph[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, start_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(graph[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], graph[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_path)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m~/tractography/data/make_simulated_neurons.py:91\u001b[0m, in \u001b[0;36mget_path\u001b[0;34m(start, boundary, kappa, rng, length, step_size, uniform_len, random_start, branch)\u001b[0m\n\u001b[1;32m     88\u001b[0m path \u001b[38;5;241m=\u001b[39m [start, q1]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length):\n\u001b[0;32m---> 91\u001b[0m     next_point \u001b[38;5;241m=\u001b[39m \u001b[43mget_next_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(next_point \u001b[38;5;241m>\u001b[39m boundary\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(next_point \u001b[38;5;241m<\u001b[39m boundary\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/tractography/data/make_simulated_neurons.py:23\u001b[0m, in \u001b[0;36mget_next_point\u001b[0;34m(q0, q1, kappa, step_size, rng)\u001b[0m\n\u001b[1;32m     21\u001b[0m last_step \u001b[38;5;241m=\u001b[39m (q1 \u001b[38;5;241m-\u001b[39m q0) \u001b[38;5;241m/\u001b[39m step_size\n\u001b[1;32m     22\u001b[0m vmf \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mvonmises_fisher(last_step, kappa)\n\u001b[0;32m---> 23\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43mvmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# step[0] = 0.0 # for paths constrained to a 2d slice\u001b[39;00m\n\u001b[1;32m     25\u001b[0m step \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m/\u001b[39m(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(step) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps)\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/scipy/stats/_multivariate.py:6967\u001b[0m, in \u001b[0;36mvonmises_fisher_frozen.rvs\u001b[0;34m(self, size, random_state)\u001b[0m\n\u001b[1;32m   6940\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Draw random variates from the Von Mises-Fisher distribution.\u001b[39;00m\n\u001b[1;32m   6941\u001b[0m \n\u001b[1;32m   6942\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6964\u001b[0m \n\u001b[1;32m   6965\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6966\u001b[0m random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist\u001b[38;5;241m.\u001b[39m_get_random_state(random_state)\n\u001b[0;32m-> 6967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rvs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6968\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/scipy/stats/_multivariate.py:6754\u001b[0m, in \u001b[0;36mvonmises_fisher_gen._rvs\u001b[0;34m(self, dim, mu, kappa, size, random_state)\u001b[0m\n\u001b[1;32m   6752\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rvs_2d(mu, kappa, size, random_state)\n\u001b[1;32m   6753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 6754\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rvs_3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6756\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rejection_sampling(dim, kappa, size,\n\u001b[1;32m   6757\u001b[0m                                        random_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/scipy/stats/_multivariate.py:6642\u001b[0m, in \u001b[0;36mvonmises_fisher_gen._rvs_3d\u001b[0;34m(self, kappa, size, random_state)\u001b[0m\n\u001b[1;32m   6640\u001b[0m temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(x))\n\u001b[1;32m   6641\u001b[0m uniformcircle \u001b[38;5;241m=\u001b[39m _sample_uniform_direction(\u001b[38;5;241m2\u001b[39m, sample_size, random_state)\n\u001b[0;32m-> 6642\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muniformcircle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6643\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muniformcircle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6644\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6646\u001b[0m     samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(samples)\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/numpy/_core/shape_base.py:361\u001b[0m, in \u001b[0;36m_stack_dispatcher\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stack_dispatcher\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    362\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m _arrays_for_stack_dispatcher(arrays)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;66;03m# optimize for the typical case where only arrays is provided\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 1000\n",
    "rng = np.random.default_rng(7)\n",
    "out_dir = \"/home/brysongray/tractography/data/training_data/branch_classifier_sim_dataset_no_artifacts_\"+date\n",
    "name=\"sim_dataset_no_artifacts\"\n",
    "random_contrast=False\n",
    "\n",
    "swc_lists = []\n",
    "for i in tqdm(range(count)):\n",
    "    swc_list = msn.make_swc_list((101,101,101),\n",
    "                                length=20,\n",
    "                                step_size=3.0,\n",
    "                                kappa=20.0,\n",
    "                                uniform_len=False,\n",
    "                                random_start=True,\n",
    "                                rng=rng,\n",
    "                                num_branches=1) # make simulated neuron paths.\n",
    "    swc_lists.append(swc_list)\n",
    "    \n",
    "sample_points = swc_random_points(50, swc_lists)\n",
    "\n",
    "annotations = {}\n",
    "obs_id = 0\n",
    "for i in tqdm(range(count)):\n",
    "    # make image from swc list\n",
    "    color = np.array([1.0, 1.0, 1.0])\n",
    "    background = np.array([0., 0., 0.])\n",
    "    if random_contrast:\n",
    "        color = rng.uniform(size=3)\n",
    "        color /= np.linalg.norm(color)\n",
    "        background = rng.uniform(size=3)\n",
    "        background = background / np.linalg.norm(background) * 0.01\n",
    "    swc_data = load_data.draw_neuron_from_swc(swc_lists[i],\n",
    "                                                width=3,\n",
    "                                                noise=0.0,\n",
    "                                                adjust=False,\n",
    "                                                neuron_color=color,\n",
    "                                                background_color=background,\n",
    "                                                random_brightness=False,\n",
    "                                                dropout=False,\n",
    "                                                binary=False,\n",
    "                                                rng=rng) # Use simulated paths to draw the image.\n",
    "    img = swc_data[\"image\"]\n",
    "    img = Image(img)\n",
    "    branch_mask = swc_data[\"branch_mask\"]\n",
    "    points = sample_points[i]\n",
    "\n",
    "    obs_dir = os.path.join(out_dir, \"observations\")\n",
    "    if not os.path.exists(obs_dir):\n",
    "            os.makedirs(obs_dir, exist_ok=True)\n",
    "\n",
    "    for point in points:\n",
    "            patch, _ = img.crop(torch.tensor(point), 7, pad=True, value=0.0)\n",
    "            i,j,k = [int(np.round(x)) for x in point]\n",
    "            label = branch_mask[0, i, j, k].item()\n",
    "            fname = f\"obs_{obs_id}.pt\"\n",
    "            torch.save(patch, os.path.join(obs_dir, fname))\n",
    "            annotations[fname] = label\n",
    "            obs_id += 1\n",
    "\n",
    "# save annotations\n",
    "df = pd.DataFrame.from_dict(annotations, orient=\"index\")\n",
    "df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_annotations.csv\"), index=False)\n",
    "# split into test and training data\n",
    "data_permutation = torch.randperm(len(annotations))\n",
    "test_idxs = data_permutation[:len(data_permutation)//5].tolist()\n",
    "training_idxs = data_permutation[len(data_permutation)//5:].tolist()\n",
    "training_annotations = {list(annotations)[i]: list(annotations.values())[i] for i in training_idxs}\n",
    "test_annotations = {list(annotations)[i]: list(annotations.values())[i] for i in test_idxs}\n",
    "# save \n",
    "df = pd.DataFrame.from_dict(training_annotations, orient=\"index\")\n",
    "df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_training_annotations.csv\"))\n",
    "df = pd.DataFrame.from_dict(test_annotations, orient=\"index\")\n",
    "df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_test_annotations.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file: /home/brysongray/data/neuromorpho/mehder/CNG version/Rasha-CA1-Exp-April-2016-right-slide-55-secion-1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/hart/CNG version/2016-10-27_541_mir-1_day_3_5.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/hart/CNG version/2016-10-27_541_mir-1_day_3_3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/briggs/CNG version/Bub_9-13_c1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/quinlan/CNG version/KQa4-12-2015-tracing.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/beining/CNG version/35dpi_ipsi_infra_06.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/kuddannaya/CNG version/Tracetest_N360_semicircle_Map2Tau_79_semi-auto_18.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/chiang/CNG version/Cha-F-000302.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/chiang/CNG version/Cha-F-600090.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/tolias/CNG version/L5MC-J130731a.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/dierssen/CNG version/WT_6mo_3_11.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/borst/CNG version/dHSN_02l.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/steelman/CNG version/gw-4-image-3_22.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/denizet/CNG version/cort4c4pg1d.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/todd/CNG version/C242-01-08-14-B1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/gaspar/CNG version/8-STRESS_2w_Female_HIP_7.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/gaspar/CNG version/1-CTR_2w_Female_Nac_6.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/gaspar/CNG version/12-DEX-STRESS_male_Nac_7.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/campos/CNG version/Astro-1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/roysam/CNG version/farsight879.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/eyewire/CNG version/skel_20220_sorted.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/luikart/CNG version/C29421-A4-60DPI-TDA-I1-CreCell-3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/defelipe/CNG version/M1KO_15.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/guizzetti/CNG version/P3_CV3_79.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/la barbera/CNG version/1-4_14.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/diniz/CNG version/C13_1-6_80um_GFAP_Z-fixed0102_08-April16_C47.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/prichard_singer/CNG version/MsJinx16_NoFlicker_1h_IBA1_NFkBinh_17.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/castanho_oliveira/CNG version/13_L3_C3_N5.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/flyem/CNG version/KC-p-5303804.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/nishitoh/CNG version/D2_C_22.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/hosseini/CNG version/Aged_mice_VAC-PBS10_58-DG-1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/jacobs/CNG version/194-2-6lw.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/giniger/CNG version/ABL-OE-14--ims.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/macdonald/CNG version/SD3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/jongbloets/CNG version/14dpi_WT2_S6_2.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/baier/CNG version/150304_2_3_d.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/baier/CNG version/20160916_BGUG_HuC_ltRFP_d7_F13.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/tenner/CNG version/7month-Arctic-C5aR1-KO-11_2.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/yayon_soreq/CNG version/Cell_134_MPD_12_FT_10_XYZ_Sorted-swc_N3DFix-swc_4.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/yayon_soreq/CNG version/Cell_528_MPD_8_FT_10_XYZ_Sorted-swc_N3DFix-swc_1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/allen cell types/CNG version/646805498_transformed.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/wong/CNG version/L100P-GSK3-Het-9.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/CN_Development_P22_F_Animal03_Trace009.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/CN_Development_P15_F_Animal01_Trace040.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_5xFAD_3mpos_F_Animal02_Trace178.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/SN_Adulthood_Control_M_Animal08_Trace005.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_CKp25_2w_M_Animal01_Trace048.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/CN_Development_P22_M_Animal03_Trace047.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_5xFAD_6mpos_F_Animal03_Trace003.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_CKp25_1w_M_Animal02_Trace067.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_CKp25_6w_F_Animal02_Trace063.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/SN_5xFAD_6mpos_M_Animal03_Trace033.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_CKp25_6w_M_Animal04_Trace008.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_5xFAD_3mpos_M_Animal03_Trace037.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/FC_Development_P15_M_Animal02_Trace109.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/FC_Development_P22_F_Animal03_Trace164.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/FC_Adulthood_Control_M_Animal01_Trace036.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_5xFAD_6mpos_M_Animal03_Trace080.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_Development_P22_M_Animal03_Trace060.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_Adulthood_Control_F_Animal01_Trace891.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/SN_Ovariectomy_Animal02_Trace052.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_Ovariectomy_Animal01_Trace032.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/OB_Adulthood_Control_F_Animal04_Trace056.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/MC-Aged-cont-S1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/SSC-Aged-control-S8.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/MC-Presymptomatic-SOD1-S3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/MC-Aged-Cont-S2.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/SSC-Aged-control-S13.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/charlet/CNG version/CeA_Astrocyte_12_003.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/charlet/CNG version/CeA_Astrocyte_3_021.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/denk/CNG version/orphan_3807.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/hamad/CNG version/pcs74_8.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/jefferis/CNG version/NNE1L.CNG.swc\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (0) must match the existing size (823) at non-singleton dimension 3.  Target sizes: [3, 15, 15, 0].  Tensor sizes: [3, 15, 15, 823]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/brysongray/data/neuromorpho/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/brysongray/tractography/data/training_data/neuromorpho_with_artifacts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_artifacts_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneurom_dataset_artifacts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 126\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(labels_dir, image_dir, out_dir, name, date)\u001b[0m\n\u001b[1;32m    124\u001b[0m branch_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbranch_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m points:\n\u001b[0;32m--> 126\u001b[0m     patch, _ \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     i,j,k \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m point]\n\u001b[1;32m    128\u001b[0m     label \u001b[38;5;241m=\u001b[39m branch_mask[\u001b[38;5;241m0\u001b[39m, i, j, k]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/tractography/data/image.py:95\u001b[0m, in \u001b[0;36mImage.crop\u001b[0;34m(self, center, radius, interp, padding_mode, pad, value)\u001b[0m\n\u001b[1;32m     93\u001b[0m     patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mradius\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m     patch_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], patch_size, patch_size, patch_size), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m value\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mpatch_\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzpad_top\u001b[49m\u001b[43m:\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mzpad_btm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mypad_front\u001b[49m\u001b[43m:\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mypad_back\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpad_left\u001b[49m\u001b[43m:\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxpad_right\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m patch\n\u001b[1;32m     96\u001b[0m     patch \u001b[38;5;241m=\u001b[39m patch_\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# patch = patch[:, 1:-1, 1:-1, 1:-1]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (0) must match the existing size (823) at non-singleton dimension 3.  Target sizes: [3, 15, 15, 0].  Tensor sizes: [3, 15, 15, 823]"
     ]
    }
   ],
   "source": [
    "collect_data(labels_dir=\"/home/brysongray/data/neuromorpho/\",\n",
    "             image_dir=\"/home/brysongray/tractography/data/training_data/neuromorpho_with_artifacts\",\n",
    "             out_dir=\"/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_artifacts_\"+date,\n",
    "             name=\"neurom_dataset_artifacts\",\n",
    "             date=date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file: /home/brysongray/data/neuromorpho/mehder/CNG version/Rasha-CA1-Exp-April-2016-right-slide-55-secion-1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/hart/CNG version/2016-10-27_541_mir-1_day_3_5.CNG.swc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6228/3193434125.py:34: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  random_point += translation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file: /home/brysongray/data/neuromorpho/hart/CNG version/2016-10-27_541_mir-1_day_3_3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/briggs/CNG version/Bub_9-13_c1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/quinlan/CNG version/KQa4-12-2015-tracing.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/beining/CNG version/35dpi_ipsi_infra_06.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/kuddannaya/CNG version/Tracetest_N360_semicircle_Map2Tau_79_semi-auto_18.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/chiang/CNG version/Cha-F-000302.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/chiang/CNG version/Cha-F-600090.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/tolias/CNG version/L5MC-J130731a.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/dierssen/CNG version/WT_6mo_3_11.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/borst/CNG version/dHSN_02l.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/steelman/CNG version/gw-4-image-3_22.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/denizet/CNG version/cort4c4pg1d.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/todd/CNG version/C242-01-08-14-B1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/gaspar/CNG version/8-STRESS_2w_Female_HIP_7.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/gaspar/CNG version/1-CTR_2w_Female_Nac_6.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/gaspar/CNG version/12-DEX-STRESS_male_Nac_7.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/campos/CNG version/Astro-1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/roysam/CNG version/farsight879.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/eyewire/CNG version/skel_20220_sorted.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/luikart/CNG version/C29421-A4-60DPI-TDA-I1-CreCell-3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/defelipe/CNG version/M1KO_15.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/guizzetti/CNG version/P3_CV3_79.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/la barbera/CNG version/1-4_14.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/diniz/CNG version/C13_1-6_80um_GFAP_Z-fixed0102_08-April16_C47.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/prichard_singer/CNG version/MsJinx16_NoFlicker_1h_IBA1_NFkBinh_17.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/castanho_oliveira/CNG version/13_L3_C3_N5.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/flyem/CNG version/KC-p-5303804.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/nishitoh/CNG version/D2_C_22.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/hosseini/CNG version/Aged_mice_VAC-PBS10_58-DG-1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/jacobs/CNG version/194-2-6lw.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/giniger/CNG version/ABL-OE-14--ims.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/macdonald/CNG version/SD3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/jongbloets/CNG version/14dpi_WT2_S6_2.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/baier/CNG version/150304_2_3_d.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/baier/CNG version/20160916_BGUG_HuC_ltRFP_d7_F13.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/tenner/CNG version/7month-Arctic-C5aR1-KO-11_2.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/yayon_soreq/CNG version/Cell_134_MPD_12_FT_10_XYZ_Sorted-swc_N3DFix-swc_4.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/yayon_soreq/CNG version/Cell_528_MPD_8_FT_10_XYZ_Sorted-swc_N3DFix-swc_1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/allen cell types/CNG version/646805498_transformed.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/wong/CNG version/L100P-GSK3-Het-9.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/CN_Development_P22_F_Animal03_Trace009.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/CN_Development_P15_F_Animal01_Trace040.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_5xFAD_3mpos_F_Animal02_Trace178.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/SN_Adulthood_Control_M_Animal08_Trace005.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_CKp25_2w_M_Animal01_Trace048.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/CN_Development_P22_M_Animal03_Trace047.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_5xFAD_6mpos_F_Animal03_Trace003.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_CKp25_1w_M_Animal02_Trace067.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_CKp25_6w_F_Animal02_Trace063.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/SN_5xFAD_6mpos_M_Animal03_Trace033.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_CKp25_6w_M_Animal04_Trace008.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_5xFAD_3mpos_M_Animal03_Trace037.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/FC_Development_P15_M_Animal02_Trace109.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/FC_Development_P22_F_Animal03_Trace164.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/FC_Adulthood_Control_M_Animal01_Trace036.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_5xFAD_6mpos_M_Animal03_Trace080.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_Development_P22_M_Animal03_Trace060.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/DG_Adulthood_Control_F_Animal01_Trace891.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/SN_Ovariectomy_Animal02_Trace052.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/S1_Ovariectomy_Animal01_Trace032.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/siegert/CNG version/OB_Adulthood_Control_F_Animal04_Trace056.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/MC-Aged-cont-S1.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/SSC-Aged-control-S8.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/MC-Presymptomatic-SOD1-S3.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/MC-Aged-Cont-S2.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/buskila/CNG version/SSC-Aged-control-S13.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/charlet/CNG version/CeA_Astrocyte_12_003.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/charlet/CNG version/CeA_Astrocyte_3_021.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/denk/CNG version/orphan_3807.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/hamad/CNG version/pcs74_8.CNG.swc\n",
      "loading file: /home/brysongray/data/neuromorpho/jefferis/CNG version/NNE1L.CNG.swc\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(7)\n",
    "\n",
    "labels_dir = \"/home/brysongray/data/neuromorpho/\"\n",
    "\n",
    "datasets = [dir for dir in os.listdir(labels_dir) if dir[0] != \".\"]\n",
    "files = []\n",
    "for dataset in datasets:\n",
    "    labels_path = os.path.join(labels_dir, dataset)\n",
    "    if \"CNG version\" in os.listdir(labels_path):\n",
    "        labels_path = labels_path + \"/CNG version\"\n",
    "    elif \"Source-Version\" in os.listdir(labels_path):\n",
    "        labels_path = labels_path + \"/Source-Version\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for f in os.listdir(labels_path):\n",
    "        if f.endswith(\"swc\") and f[0] != \".\":\n",
    "            files.append(os.path.join(labels_path, f))\n",
    "\n",
    "samples_per_file = 1000\n",
    "sample_points = {}\n",
    "for labels_file in files:\n",
    "    fname = labels_file.split(\"/\")[-1].split(\".CNG\")[0]\n",
    "    swc_list = load_data.read_swc(labels_file)\n",
    "    sections, section_graph, branches, terminals, scale = load_data.parse_swc_list(swc_list, adjust=True)\n",
    "    rand_sections = rng.choice(list(sections.keys()), size=samples_per_file)\n",
    "    points = []\n",
    "    for i in rand_sections:\n",
    "        section_flat = sections[i].flatten(0,1).detach().clone() # type: ignore # \n",
    "        random_point = rng.choice(np.arange(len(section_flat)))\n",
    "        random_point = section_flat[random_point]\n",
    "        # random translation vector from normal distribution about random_point\n",
    "        translation = rng.uniform(low=0.0, high=1.0, size=(3,))*8.0 - 4.0\n",
    "        random_point += translation\n",
    "        points.append(random_point)\n",
    "    points = torch.stack(points)\n",
    "    sample_points[fname] = points\n",
    "\n",
    "torch.save(sample_points, \"/home/brysongray/tractography/data/sample_points.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"/home/brysongray/tractography/neuron_trx/training_data/neuromorpho\"\n",
    "image_files = os.listdir(image_dir)\n",
    "f = image_files[0]\n",
    "data = torch.load(os.path.join(image_dir, f), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sample_points, \"sample_points.pt\")\n",
    "sample_points = torch.load(\"sample_points.pt\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SN_Adulthood_Control_M_Animal08_Trace005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6228/1802225951.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  points = torch.tensor(sample_points[fname])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skel_20220_sorted\n",
      "SN_5xFAD_6mpos_M_Animal03_Trace033\n",
      "1-CTR_2w_Female_Nac_6\n",
      "Tracetest_N360_semicircle_Map2Tau_79_semi-auto_18\n",
      "FC_Development_P15_M_Animal02_Trace109\n",
      "CeA_Astrocyte_3_021\n",
      "646805498_transformed\n",
      "SD3\n",
      "14dpi_WT2_S6_2\n",
      "C242-01-08-14-B1\n",
      "20160916_BGUG_HuC_ltRFP_d7_F13\n",
      "2016-10-27_541_mir-1_day_3_5\n",
      "cort4c4pg1d\n",
      "Cha-F-000302\n",
      "CN_Development_P22_F_Animal03_Trace009\n",
      "pcs74_8\n",
      "P3_CV3_79\n",
      "S1_CKp25_1w_M_Animal02_Trace067\n",
      "SN_Ovariectomy_Animal02_Trace052\n",
      "MC-Aged-Cont-S2\n",
      "L5MC-J130731a\n",
      "Bub_9-13_c1\n",
      "S1_5xFAD_3mpos_M_Animal03_Trace037\n",
      "FC_Development_P22_F_Animal03_Trace164\n",
      "Cell_134_MPD_12_FT_10_XYZ_Sorted-swc_N3DFix-swc_4\n",
      "KC-p-5303804\n",
      "L100P-GSK3-Het-9\n",
      "8-STRESS_2w_Female_HIP_7\n",
      "farsight879\n",
      "13_L3_C3_N5\n",
      "Cell_528_MPD_8_FT_10_XYZ_Sorted-swc_N3DFix-swc_1\n",
      "7month-Arctic-C5aR1-KO-11_2\n",
      "S1_CKp25_6w_M_Animal04_Trace008\n",
      "2016-10-27_541_mir-1_day_3_3\n",
      "orphan_3807\n",
      "WT_6mo_3_11\n",
      "194-2-6lw\n",
      "DG_5xFAD_3mpos_F_Animal02_Trace178\n",
      "D2_C_22\n",
      "FC_Adulthood_Control_M_Animal01_Trace036\n",
      "Aged_mice_VAC-PBS10_58-DG-1\n",
      "S1_CKp25_6w_F_Animal02_Trace063\n",
      "35dpi_ipsi_infra_06\n",
      "C13_1-6_80um_GFAP_Z-fixed0102_08-April16_C47\n",
      "NNE1L\n",
      "MC-Presymptomatic-SOD1-S3\n",
      "DG_CKp25_2w_M_Animal01_Trace048\n",
      "DG_Development_P22_M_Animal03_Trace060\n",
      "M1KO_15\n",
      "DG_Adulthood_Control_F_Animal01_Trace891\n",
      "Astro-1\n",
      "CeA_Astrocyte_12_003\n",
      "MC-Aged-cont-S1\n",
      "Cha-F-600090\n",
      "gw-4-image-3_22\n",
      "150304_2_3_d\n",
      "S1_5xFAD_6mpos_F_Animal03_Trace003\n",
      "MsJinx16_NoFlicker_1h_IBA1_NFkBinh_17\n",
      "C29421-A4-60DPI-TDA-I1-CreCell-3\n",
      "SSC-Aged-control-S13\n",
      "dHSN_02l\n",
      "12-DEX-STRESS_male_Nac_7\n",
      "SSC-Aged-control-S8\n",
      "CN_Development_P15_F_Animal01_Trace040\n",
      "ABL-OE-14--ims\n",
      "OB_Adulthood_Control_F_Animal04_Trace056\n",
      "1-4_14\n",
      "CN_Development_P22_M_Animal03_Trace047\n",
      "KQa4-12-2015-tracing\n",
      "S1_Ovariectomy_Animal01_Trace032\n",
      "DG_5xFAD_6mpos_M_Animal03_Trace080\n",
      "Rasha-CA1-Exp-April-2016-right-slide-55-secion-1\n"
     ]
    }
   ],
   "source": [
    "sample_points = torch.load(\"sample_points.pt\", weights_only=True)\n",
    "\n",
    "name = \"neurom_dataset_with_artifacts\"\n",
    "image_dir = \"/home/brysongray/tractography/data/training_data/neuromorpho_with_artifacts\"\n",
    "out_dir = f\"/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_artifacts_{date}\"\n",
    "os.makedirs(os.path.join(out_dir,\"observations\"), exist_ok=True)\n",
    "image_files = os.listdir(image_dir)\n",
    "annotations = {}\n",
    "obs_id = 0\n",
    "for f in image_files:\n",
    "    fname = f.split(\".CNG\")[0]\n",
    "    print(fname)\n",
    "    points = torch.tensor(sample_points[fname])\n",
    "    data = torch.load(os.path.join(image_dir, f), weights_only=True)\n",
    "    img = data[\"image\"]\n",
    "    img = Image(img)\n",
    "    branch_mask = data[\"branch_mask\"]\n",
    "    for point in points:\n",
    "        # point = torch.round(point).to(dtype=torch.int)\n",
    "        patch, _ = img.crop(point, 7, pad=True, value=0.0)\n",
    "        i,j,k = [int(torch.floor(x)) for x in point]\n",
    "        label = branch_mask[0, i, j, k].item()\n",
    "        fname = f\"obs_{obs_id}.pt\"\n",
    "        torch.save(patch, os.path.join(os.path.join(out_dir, \"observations\"), fname))\n",
    "        annotations[fname] = label\n",
    "        obs_id += 1\n",
    "\n",
    "# save annotations\n",
    "df = pd.DataFrame.from_dict(annotations, orient=\"index\")\n",
    "df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_annotations.csv\"), index=False)\n",
    "# split into test and training data\n",
    "data_permutation = torch.randperm(len(annotations))\n",
    "test_idxs = data_permutation[:len(data_permutation)//5].tolist()\n",
    "training_idxs = data_permutation[len(data_permutation)//5:].tolist()\n",
    "training_annotations = {list(annotations)[i]: list(annotations.values())[i] for i in training_idxs}\n",
    "test_annotations = {list(annotations)[i]: list(annotations.values())[i] for i in test_idxs}\n",
    "# save \n",
    "df = pd.DataFrame.from_dict(training_annotations, orient=\"index\")\n",
    "df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_training_annotations.csv\"))\n",
    "df = pd.DataFrame.from_dict(test_annotations, orient=\"index\")\n",
    "df.to_csv(os.path.join(out_dir, f\"branch_classifier_{name}_{date}_test_annotations.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dir = \"/home/brysongray/tractography/neuron_trx/training_data/branch_classifier_neurom_dataset_1_1-7-25_15/observations\"\n",
    "# dir = \"/home/brysongray/tractography/data/training_data/branch_classifier_sim_dataset_artifacts_01-24-25/observations\"\n",
    "dir = \"/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_artifacts_01-24-25/observations\"\n",
    "\n",
    "files = os.listdir(dir)\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(12,12))\n",
    "for i,f in enumerate(files[:9]):\n",
    "    img = torch.load(os.path.join(dir, f), weights_only=True)\n",
    "    ax = ax.ravel()\n",
    "    ax[i].imshow(img[0].amax(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset class\n",
    "class StateData(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) # type: ignore\n",
    "        image = torch.load(img_path, weights_only=True)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "# define transforms\n",
    "def transform(image):\n",
    "    perm = torch.randperm(3) + 1\n",
    "    image = image.permute([0,*perm])\n",
    "    if torch.rand(1)>0.5: image = image.flip(-1)\n",
    "    if torch.rand(1)>0.5: image = image.flip(-2)\n",
    "    if torch.rand(1)>0.5: image = image.flip(-3)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    positive_count = len(np.where(dataloader.dataset.img_labels.iloc[:,1] > 0.0)[0])\n",
    "    negative_count = size - positive_count\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        out = model(X[:,:3].to(device=DEVICE))\n",
    "        out = torch.nn.functional.sigmoid(out.squeeze())\n",
    "        # out = torch.nn.functional.softmax(out, dim=1)\n",
    "        # y = torch.nn.functional.one_hot(y, num_classes=3)\n",
    "        y = y.to(dtype=torch.float, device=DEVICE)\n",
    "        weights = torch.where(y > 0.0, positive_count/size, negative_count/size)\n",
    "        loss = loss_fn(out,y)\n",
    "        loss = torch.mean(loss * weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X) + len(X)\n",
    "            losses.append(loss)\n",
    "            accuracy = ((out > 0.5) == y).type(torch.float).sum().item()\n",
    "            accuracy = accuracy / len(y) * 100\n",
    "            print(f\"Accuracy: {accuracy}, Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    positive_count = len(np.where(dataloader.dataset.img_labels.iloc[:,1] > 0.0)[0])\n",
    "    negative_count = size - positive_count\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, TP, TN, FP, FN = 0,0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            out = model(X[:,:3].to(device=DEVICE))\n",
    "            out = torch.nn.functional.sigmoid(out.squeeze())\n",
    "            # out = torch.nn.functional.softmax(out, dim=1)\n",
    "            # y = torch.nn.functional.one_hot(y, num_classes=3)\n",
    "            y = y.to(dtype=torch.float, device=DEVICE)\n",
    "            weights = torch.where(y > 0.0, positive_count/size, negative_count/size)\n",
    "            loss = loss_fn(out,y)\n",
    "            loss = torch.mean(loss * weights)\n",
    "            test_loss += loss.item()\n",
    "            # correct += (torch.argmax(out, dim=1) == torch.argmax(y, dim=1)).type(torch.float).sum().item()\n",
    "            # correct += ((out > 0.5) == y).type(torch.float).sum().item()\n",
    "            threshold = 0.5\n",
    "            TP_ = ((out > threshold) & (y > 0.0)).type(torch.float).sum().item()\n",
    "            TN_ = ((out <= threshold) & (y <= 0.0)).type(torch.float).sum().item()\n",
    "            FP_ = ((out > threshold).sum().item() - TP_)\n",
    "            FN_ = ((out <= threshold).sum().item() - TN_)\n",
    "            TP += TP_\n",
    "            TN += TN_\n",
    "            FP += FP_\n",
    "            FN += FN_\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    correct = (TP + TN) / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\\\n",
    "           Precision: {precision:>0.3f}, Recall: {recall:>0.3f}\")\n",
    "    # print(f\"Avg MSE loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = StateData(annotations_file=\"/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_no_artifacts_01-24-25/branch_classifier_neurom_dataset_no_artifacts_01-24-25_training_annotations.csv\",\n",
    "                          img_dir=\"/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_no_artifacts_01-24-25/observations\",\n",
    "                          transform=transform)\n",
    "test_data = StateData(annotations_file=\"/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_no_artifacts_01-24-25/branch_classifier_neurom_dataset_no_artifacts_01-24-25_test_annotations.csv\",\n",
    "                          img_dir=\"/home/brysongray/tractography/data/training_data/branch_classifier_neurom_dataset_no_artifacts_01-24-25/observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_sample_count = np.sum(training_data.img_labels.iloc[:,1] > 0.0)\n",
    "nonzero_weight = 1. / nonzero_sample_count\n",
    "zero_sample_count = len(training_data.img_labels) - nonzero_sample_count\n",
    "zero_weight = 1. / zero_sample_count\n",
    "training_samples_weight = [nonzero_weight if t > 0.0 else zero_weight for t in training_data.img_labels.iloc[:,1]]\n",
    "training_sampler = WeightedRandomSampler(training_samples_weight, len(training_samples_weight))\n",
    "\n",
    "nonzero_sample_count = np.sum(test_data.img_labels.iloc[:,1] > 0.0)\n",
    "nonzero_weight = 1. / nonzero_sample_count\n",
    "zero_weight = 1. / (len(test_data.img_labels) - nonzero_sample_count)\n",
    "test_samples_weight = [nonzero_weight if t > 0.0 else zero_weight for t in test_data.img_labels.iloc[:,1]]\n",
    "test_sampler = WeightedRandomSampler(test_samples_weight, len(test_samples_weight))\n",
    "\n",
    "# initialize dataloaders\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, sampler=training_sampler)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Accuracy: 57.8125, Loss: 0.271621  [   64/57600]\n",
      "Accuracy: 71.875, Loss: 1.991424  [  704/57600]\n",
      "Accuracy: 82.8125, Loss: 0.169316  [ 1344/57600]\n",
      "Accuracy: 78.125, Loss: 0.318538  [ 1984/57600]\n",
      "Accuracy: 75.0, Loss: 0.270964  [ 2624/57600]\n",
      "Accuracy: 75.0, Loss: 0.292223  [ 3264/57600]\n",
      "Accuracy: 84.375, Loss: 0.256320  [ 3904/57600]\n",
      "Accuracy: 81.25, Loss: 0.187409  [ 4544/57600]\n",
      "Accuracy: 82.8125, Loss: 0.182644  [ 5184/57600]\n",
      "Accuracy: 81.25, Loss: 0.201644  [ 5824/57600]\n",
      "Accuracy: 81.25, Loss: 0.179003  [ 6464/57600]\n",
      "Accuracy: 84.375, Loss: 0.224823  [ 7104/57600]\n",
      "Accuracy: 89.0625, Loss: 0.126321  [ 7744/57600]\n",
      "Accuracy: 81.25, Loss: 0.191251  [ 8384/57600]\n",
      "Accuracy: 76.5625, Loss: 0.260141  [ 9024/57600]\n",
      "Accuracy: 81.25, Loss: 0.188079  [ 9664/57600]\n",
      "Accuracy: 85.9375, Loss: 0.163575  [10304/57600]\n",
      "Accuracy: 85.9375, Loss: 0.172764  [10944/57600]\n",
      "Accuracy: 81.25, Loss: 0.179298  [11584/57600]\n",
      "Accuracy: 78.125, Loss: 0.227476  [12224/57600]\n",
      "Accuracy: 78.125, Loss: 0.239407  [12864/57600]\n",
      "Accuracy: 79.6875, Loss: 0.185083  [13504/57600]\n",
      "Accuracy: 85.9375, Loss: 0.188088  [14144/57600]\n",
      "Accuracy: 82.8125, Loss: 0.182880  [14784/57600]\n",
      "Accuracy: 82.8125, Loss: 0.213436  [15424/57600]\n",
      "Accuracy: 85.9375, Loss: 0.155040  [16064/57600]\n",
      "Accuracy: 79.6875, Loss: 0.186551  [16704/57600]\n",
      "Accuracy: 81.25, Loss: 0.186564  [17344/57600]\n",
      "Accuracy: 81.25, Loss: 0.223715  [17984/57600]\n",
      "Accuracy: 84.375, Loss: 0.191946  [18624/57600]\n",
      "Accuracy: 93.75, Loss: 0.105965  [19264/57600]\n",
      "Accuracy: 84.375, Loss: 0.210989  [19904/57600]\n",
      "Accuracy: 78.125, Loss: 0.237199  [20544/57600]\n",
      "Accuracy: 81.25, Loss: 0.209668  [21184/57600]\n",
      "Accuracy: 84.375, Loss: 0.175250  [21824/57600]\n",
      "Accuracy: 81.25, Loss: 0.177035  [22464/57600]\n",
      "Accuracy: 81.25, Loss: 0.179025  [23104/57600]\n",
      "Accuracy: 84.375, Loss: 0.204948  [23744/57600]\n",
      "Accuracy: 81.25, Loss: 0.163658  [24384/57600]\n",
      "Accuracy: 90.625, Loss: 0.132812  [25024/57600]\n",
      "Accuracy: 82.8125, Loss: 0.201107  [25664/57600]\n",
      "Accuracy: 84.375, Loss: 0.190812  [26304/57600]\n",
      "Accuracy: 85.9375, Loss: 0.156176  [26944/57600]\n",
      "Accuracy: 81.25, Loss: 0.209492  [27584/57600]\n",
      "Accuracy: 82.8125, Loss: 0.234721  [28224/57600]\n",
      "Accuracy: 85.9375, Loss: 0.151434  [28864/57600]\n",
      "Accuracy: 79.6875, Loss: 0.185820  [29504/57600]\n",
      "Accuracy: 79.6875, Loss: 0.241435  [30144/57600]\n",
      "Accuracy: 76.5625, Loss: 0.206758  [30784/57600]\n",
      "Accuracy: 85.9375, Loss: 0.132165  [31424/57600]\n",
      "Accuracy: 84.375, Loss: 0.197249  [32064/57600]\n",
      "Accuracy: 70.3125, Loss: 0.288179  [32704/57600]\n",
      "Accuracy: 84.375, Loss: 0.180451  [33344/57600]\n",
      "Accuracy: 90.625, Loss: 0.119326  [33984/57600]\n",
      "Accuracy: 79.6875, Loss: 0.248230  [34624/57600]\n",
      "Accuracy: 87.5, Loss: 0.144746  [35264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.158615  [35904/57600]\n",
      "Accuracy: 79.6875, Loss: 0.191922  [36544/57600]\n",
      "Accuracy: 85.9375, Loss: 0.154301  [37184/57600]\n",
      "Accuracy: 79.6875, Loss: 0.189162  [37824/57600]\n",
      "Accuracy: 87.5, Loss: 0.155752  [38464/57600]\n",
      "Accuracy: 85.9375, Loss: 0.199821  [39104/57600]\n",
      "Accuracy: 85.9375, Loss: 0.166556  [39744/57600]\n",
      "Accuracy: 79.6875, Loss: 0.215774  [40384/57600]\n",
      "Accuracy: 85.9375, Loss: 0.153660  [41024/57600]\n",
      "Accuracy: 81.25, Loss: 0.226190  [41664/57600]\n",
      "Accuracy: 85.9375, Loss: 0.146312  [42304/57600]\n",
      "Accuracy: 81.25, Loss: 0.242256  [42944/57600]\n",
      "Accuracy: 79.6875, Loss: 0.192288  [43584/57600]\n",
      "Accuracy: 85.9375, Loss: 0.150235  [44224/57600]\n",
      "Accuracy: 93.75, Loss: 0.140320  [44864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.181825  [45504/57600]\n",
      "Accuracy: 79.6875, Loss: 0.226814  [46144/57600]\n",
      "Accuracy: 70.3125, Loss: 0.301348  [46784/57600]\n",
      "Accuracy: 85.9375, Loss: 0.162849  [47424/57600]\n",
      "Accuracy: 73.4375, Loss: 0.297959  [48064/57600]\n",
      "Accuracy: 82.8125, Loss: 0.144031  [48704/57600]\n",
      "Accuracy: 73.4375, Loss: 0.186569  [49344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.155900  [49984/57600]\n",
      "Accuracy: 84.375, Loss: 0.135672  [50624/57600]\n",
      "Accuracy: 81.25, Loss: 0.196255  [51264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.129383  [51904/57600]\n",
      "Accuracy: 85.9375, Loss: 0.167424  [52544/57600]\n",
      "Accuracy: 87.5, Loss: 0.169579  [53184/57600]\n",
      "Accuracy: 76.5625, Loss: 0.253244  [53824/57600]\n",
      "Accuracy: 90.625, Loss: 0.124780  [54464/57600]\n",
      "Accuracy: 79.6875, Loss: 0.186672  [55104/57600]\n",
      "Accuracy: 78.125, Loss: 0.230944  [55744/57600]\n",
      "Accuracy: 90.625, Loss: 0.159513  [56384/57600]\n",
      "Accuracy: 84.375, Loss: 0.157090  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.158799 \n",
      "           Precision: 0.826, Recall: 0.934\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Accuracy: 85.9375, Loss: 0.154542  [   64/57600]\n",
      "Accuracy: 85.9375, Loss: 0.161805  [  704/57600]\n",
      "Accuracy: 87.5, Loss: 0.178660  [ 1344/57600]\n",
      "Accuracy: 84.375, Loss: 0.148646  [ 1984/57600]\n",
      "Accuracy: 78.125, Loss: 0.191936  [ 2624/57600]\n",
      "Accuracy: 82.8125, Loss: 0.168526  [ 3264/57600]\n",
      "Accuracy: 84.375, Loss: 0.191190  [ 3904/57600]\n",
      "Accuracy: 82.8125, Loss: 0.165341  [ 4544/57600]\n",
      "Accuracy: 84.375, Loss: 0.223855  [ 5184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.142112  [ 5824/57600]\n",
      "Accuracy: 93.75, Loss: 0.109261  [ 6464/57600]\n",
      "Accuracy: 82.8125, Loss: 0.130778  [ 7104/57600]\n",
      "Accuracy: 78.125, Loss: 0.236356  [ 7744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.177352  [ 8384/57600]\n",
      "Accuracy: 82.8125, Loss: 0.194951  [ 9024/57600]\n",
      "Accuracy: 79.6875, Loss: 0.174699  [ 9664/57600]\n",
      "Accuracy: 82.8125, Loss: 0.184152  [10304/57600]\n",
      "Accuracy: 81.25, Loss: 0.281152  [10944/57600]\n",
      "Accuracy: 89.0625, Loss: 0.127242  [11584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113330  [12224/57600]\n",
      "Accuracy: 75.0, Loss: 0.198707  [12864/57600]\n",
      "Accuracy: 76.5625, Loss: 0.262006  [13504/57600]\n",
      "Accuracy: 82.8125, Loss: 0.183496  [14144/57600]\n",
      "Accuracy: 87.5, Loss: 0.191680  [14784/57600]\n",
      "Accuracy: 90.625, Loss: 0.122258  [15424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.092537  [16064/57600]\n",
      "Accuracy: 89.0625, Loss: 0.118910  [16704/57600]\n",
      "Accuracy: 87.5, Loss: 0.169779  [17344/57600]\n",
      "Accuracy: 84.375, Loss: 0.193483  [17984/57600]\n",
      "Accuracy: 84.375, Loss: 0.179648  [18624/57600]\n",
      "Accuracy: 84.375, Loss: 0.169439  [19264/57600]\n",
      "Accuracy: 93.75, Loss: 0.123165  [19904/57600]\n",
      "Accuracy: 87.5, Loss: 0.158456  [20544/57600]\n",
      "Accuracy: 89.0625, Loss: 0.141031  [21184/57600]\n",
      "Accuracy: 93.75, Loss: 0.136432  [21824/57600]\n",
      "Accuracy: 82.8125, Loss: 0.262267  [22464/57600]\n",
      "Accuracy: 78.125, Loss: 0.206967  [23104/57600]\n",
      "Accuracy: 87.5, Loss: 0.138858  [23744/57600]\n",
      "Accuracy: 78.125, Loss: 0.221179  [24384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.128511  [25024/57600]\n",
      "Accuracy: 89.0625, Loss: 0.122687  [25664/57600]\n",
      "Accuracy: 85.9375, Loss: 0.191210  [26304/57600]\n",
      "Accuracy: 87.5, Loss: 0.128378  [26944/57600]\n",
      "Accuracy: 93.75, Loss: 0.110753  [27584/57600]\n",
      "Accuracy: 96.875, Loss: 0.058824  [28224/57600]\n",
      "Accuracy: 89.0625, Loss: 0.148769  [28864/57600]\n",
      "Accuracy: 81.25, Loss: 0.190349  [29504/57600]\n",
      "Accuracy: 78.125, Loss: 0.190252  [30144/57600]\n",
      "Accuracy: 84.375, Loss: 0.221934  [30784/57600]\n",
      "Accuracy: 87.5, Loss: 0.160848  [31424/57600]\n",
      "Accuracy: 87.5, Loss: 0.120931  [32064/57600]\n",
      "Accuracy: 79.6875, Loss: 0.198669  [32704/57600]\n",
      "Accuracy: 82.8125, Loss: 0.172022  [33344/57600]\n",
      "Accuracy: 85.9375, Loss: 0.132120  [33984/57600]\n",
      "Accuracy: 84.375, Loss: 0.198287  [34624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.134961  [35264/57600]\n",
      "Accuracy: 96.875, Loss: 0.111644  [35904/57600]\n",
      "Accuracy: 93.75, Loss: 0.092125  [36544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.097995  [37184/57600]\n",
      "Accuracy: 84.375, Loss: 0.176939  [37824/57600]\n",
      "Accuracy: 85.9375, Loss: 0.223506  [38464/57600]\n",
      "Accuracy: 81.25, Loss: 0.190978  [39104/57600]\n",
      "Accuracy: 87.5, Loss: 0.126764  [39744/57600]\n",
      "Accuracy: 81.25, Loss: 0.225834  [40384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.168985  [41024/57600]\n",
      "Accuracy: 87.5, Loss: 0.169205  [41664/57600]\n",
      "Accuracy: 90.625, Loss: 0.152566  [42304/57600]\n",
      "Accuracy: 82.8125, Loss: 0.191399  [42944/57600]\n",
      "Accuracy: 84.375, Loss: 0.172437  [43584/57600]\n",
      "Accuracy: 81.25, Loss: 0.240822  [44224/57600]\n",
      "Accuracy: 90.625, Loss: 0.109880  [44864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.143012  [45504/57600]\n",
      "Accuracy: 90.625, Loss: 0.153884  [46144/57600]\n",
      "Accuracy: 89.0625, Loss: 0.172531  [46784/57600]\n",
      "Accuracy: 82.8125, Loss: 0.217255  [47424/57600]\n",
      "Accuracy: 82.8125, Loss: 0.189758  [48064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.123892  [48704/57600]\n",
      "Accuracy: 89.0625, Loss: 0.185906  [49344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.154247  [49984/57600]\n",
      "Accuracy: 82.8125, Loss: 0.193137  [50624/57600]\n",
      "Accuracy: 90.625, Loss: 0.121521  [51264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.118837  [51904/57600]\n",
      "Accuracy: 82.8125, Loss: 0.216173  [52544/57600]\n",
      "Accuracy: 87.5, Loss: 0.165770  [53184/57600]\n",
      "Accuracy: 85.9375, Loss: 0.148555  [53824/57600]\n",
      "Accuracy: 85.9375, Loss: 0.163848  [54464/57600]\n",
      "Accuracy: 87.5, Loss: 0.132411  [55104/57600]\n",
      "Accuracy: 90.625, Loss: 0.108949  [55744/57600]\n",
      "Accuracy: 81.25, Loss: 0.261551  [56384/57600]\n",
      "Accuracy: 84.375, Loss: 0.171840  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.147168 \n",
      "           Precision: 0.863, Recall: 0.914\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Accuracy: 84.375, Loss: 0.150180  [   64/57600]\n",
      "Accuracy: 87.5, Loss: 0.170200  [  704/57600]\n",
      "Accuracy: 87.5, Loss: 0.140927  [ 1344/57600]\n",
      "Accuracy: 95.3125, Loss: 0.060722  [ 1984/57600]\n",
      "Accuracy: 84.375, Loss: 0.216012  [ 2624/57600]\n",
      "Accuracy: 87.5, Loss: 0.171364  [ 3264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.157645  [ 3904/57600]\n",
      "Accuracy: 90.625, Loss: 0.123614  [ 4544/57600]\n",
      "Accuracy: 85.9375, Loss: 0.168577  [ 5184/57600]\n",
      "Accuracy: 78.125, Loss: 0.276736  [ 5824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.125069  [ 6464/57600]\n",
      "Accuracy: 90.625, Loss: 0.116607  [ 7104/57600]\n",
      "Accuracy: 85.9375, Loss: 0.129814  [ 7744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.184603  [ 8384/57600]\n",
      "Accuracy: 87.5, Loss: 0.155579  [ 9024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.112654  [ 9664/57600]\n",
      "Accuracy: 85.9375, Loss: 0.170482  [10304/57600]\n",
      "Accuracy: 89.0625, Loss: 0.129113  [10944/57600]\n",
      "Accuracy: 90.625, Loss: 0.140192  [11584/57600]\n",
      "Accuracy: 89.0625, Loss: 0.127566  [12224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.107515  [12864/57600]\n",
      "Accuracy: 84.375, Loss: 0.193875  [13504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.158827  [14144/57600]\n",
      "Accuracy: 95.3125, Loss: 0.085432  [14784/57600]\n",
      "Accuracy: 98.4375, Loss: 0.054522  [15424/57600]\n",
      "Accuracy: 85.9375, Loss: 0.163140  [16064/57600]\n",
      "Accuracy: 78.125, Loss: 0.190710  [16704/57600]\n",
      "Accuracy: 87.5, Loss: 0.136624  [17344/57600]\n",
      "Accuracy: 90.625, Loss: 0.114954  [17984/57600]\n",
      "Accuracy: 85.9375, Loss: 0.196711  [18624/57600]\n",
      "Accuracy: 87.5, Loss: 0.149481  [19264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.123541  [19904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.153345  [20544/57600]\n",
      "Accuracy: 87.5, Loss: 0.206918  [21184/57600]\n",
      "Accuracy: 85.9375, Loss: 0.150298  [21824/57600]\n",
      "Accuracy: 87.5, Loss: 0.157836  [22464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.107609  [23104/57600]\n",
      "Accuracy: 87.5, Loss: 0.126015  [23744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.125198  [24384/57600]\n",
      "Accuracy: 87.5, Loss: 0.120228  [25024/57600]\n",
      "Accuracy: 98.4375, Loss: 0.066902  [25664/57600]\n",
      "Accuracy: 85.9375, Loss: 0.168734  [26304/57600]\n",
      "Accuracy: 84.375, Loss: 0.189542  [26944/57600]\n",
      "Accuracy: 87.5, Loss: 0.113460  [27584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.101686  [28224/57600]\n",
      "Accuracy: 84.375, Loss: 0.147190  [28864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.110822  [29504/57600]\n",
      "Accuracy: 85.9375, Loss: 0.144035  [30144/57600]\n",
      "Accuracy: 90.625, Loss: 0.123352  [30784/57600]\n",
      "Accuracy: 85.9375, Loss: 0.152499  [31424/57600]\n",
      "Accuracy: 89.0625, Loss: 0.169730  [32064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.110945  [32704/57600]\n",
      "Accuracy: 87.5, Loss: 0.191004  [33344/57600]\n",
      "Accuracy: 85.9375, Loss: 0.144066  [33984/57600]\n",
      "Accuracy: 87.5, Loss: 0.171826  [34624/57600]\n",
      "Accuracy: 90.625, Loss: 0.154490  [35264/57600]\n",
      "Accuracy: 79.6875, Loss: 0.194100  [35904/57600]\n",
      "Accuracy: 84.375, Loss: 0.180764  [36544/57600]\n",
      "Accuracy: 90.625, Loss: 0.121250  [37184/57600]\n",
      "Accuracy: 87.5, Loss: 0.197978  [37824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.178932  [38464/57600]\n",
      "Accuracy: 93.75, Loss: 0.107720  [39104/57600]\n",
      "Accuracy: 85.9375, Loss: 0.135784  [39744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.144662  [40384/57600]\n",
      "Accuracy: 82.8125, Loss: 0.163888  [41024/57600]\n",
      "Accuracy: 89.0625, Loss: 0.150977  [41664/57600]\n",
      "Accuracy: 90.625, Loss: 0.137816  [42304/57600]\n",
      "Accuracy: 90.625, Loss: 0.143815  [42944/57600]\n",
      "Accuracy: 89.0625, Loss: 0.207831  [43584/57600]\n",
      "Accuracy: 89.0625, Loss: 0.104368  [44224/57600]\n",
      "Accuracy: 84.375, Loss: 0.226491  [44864/57600]\n",
      "Accuracy: 87.5, Loss: 0.150120  [45504/57600]\n",
      "Accuracy: 93.75, Loss: 0.094915  [46144/57600]\n",
      "Accuracy: 87.5, Loss: 0.148861  [46784/57600]\n",
      "Accuracy: 90.625, Loss: 0.116501  [47424/57600]\n",
      "Accuracy: 90.625, Loss: 0.133626  [48064/57600]\n",
      "Accuracy: 93.75, Loss: 0.105959  [48704/57600]\n",
      "Accuracy: 90.625, Loss: 0.143103  [49344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.121678  [49984/57600]\n",
      "Accuracy: 82.8125, Loss: 0.196096  [50624/57600]\n",
      "Accuracy: 93.75, Loss: 0.100161  [51264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.132381  [51904/57600]\n",
      "Accuracy: 90.625, Loss: 0.152665  [52544/57600]\n",
      "Accuracy: 84.375, Loss: 0.207653  [53184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.101828  [53824/57600]\n",
      "Accuracy: 89.0625, Loss: 0.117739  [54464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.137842  [55104/57600]\n",
      "Accuracy: 82.8125, Loss: 0.214265  [55744/57600]\n",
      "Accuracy: 90.625, Loss: 0.114441  [56384/57600]\n",
      "Accuracy: 92.1875, Loss: 0.115523  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.228136 \n",
      "           Precision: 0.889, Recall: 0.697\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Accuracy: 89.0625, Loss: 0.132975  [   64/57600]\n",
      "Accuracy: 89.0625, Loss: 0.151636  [  704/57600]\n",
      "Accuracy: 87.5, Loss: 0.112228  [ 1344/57600]\n",
      "Accuracy: 90.625, Loss: 0.100002  [ 1984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.159525  [ 2624/57600]\n",
      "Accuracy: 89.0625, Loss: 0.175526  [ 3264/57600]\n",
      "Accuracy: 93.75, Loss: 0.134785  [ 3904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.125817  [ 4544/57600]\n",
      "Accuracy: 87.5, Loss: 0.171220  [ 5184/57600]\n",
      "Accuracy: 87.5, Loss: 0.129822  [ 5824/57600]\n",
      "Accuracy: 95.3125, Loss: 0.061773  [ 6464/57600]\n",
      "Accuracy: 96.875, Loss: 0.052453  [ 7104/57600]\n",
      "Accuracy: 82.8125, Loss: 0.175427  [ 7744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.141803  [ 8384/57600]\n",
      "Accuracy: 92.1875, Loss: 0.123229  [ 9024/57600]\n",
      "Accuracy: 89.0625, Loss: 0.109958  [ 9664/57600]\n",
      "Accuracy: 82.8125, Loss: 0.185506  [10304/57600]\n",
      "Accuracy: 84.375, Loss: 0.273436  [10944/57600]\n",
      "Accuracy: 82.8125, Loss: 0.187577  [11584/57600]\n",
      "Accuracy: 85.9375, Loss: 0.195155  [12224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.118793  [12864/57600]\n",
      "Accuracy: 93.75, Loss: 0.111782  [13504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.127552  [14144/57600]\n",
      "Accuracy: 85.9375, Loss: 0.156721  [14784/57600]\n",
      "Accuracy: 87.5, Loss: 0.150908  [15424/57600]\n",
      "Accuracy: 89.0625, Loss: 0.122183  [16064/57600]\n",
      "Accuracy: 87.5, Loss: 0.143946  [16704/57600]\n",
      "Accuracy: 85.9375, Loss: 0.161011  [17344/57600]\n",
      "Accuracy: 78.125, Loss: 0.216063  [17984/57600]\n",
      "Accuracy: 90.625, Loss: 0.135546  [18624/57600]\n",
      "Accuracy: 93.75, Loss: 0.080715  [19264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.159667  [19904/57600]\n",
      "Accuracy: 85.9375, Loss: 0.151154  [20544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.129934  [21184/57600]\n",
      "Accuracy: 96.875, Loss: 0.066811  [21824/57600]\n",
      "Accuracy: 84.375, Loss: 0.204650  [22464/57600]\n",
      "Accuracy: 85.9375, Loss: 0.163635  [23104/57600]\n",
      "Accuracy: 81.25, Loss: 0.201034  [23744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.085921  [24384/57600]\n",
      "Accuracy: 82.8125, Loss: 0.167659  [25024/57600]\n",
      "Accuracy: 93.75, Loss: 0.098152  [25664/57600]\n",
      "Accuracy: 93.75, Loss: 0.076166  [26304/57600]\n",
      "Accuracy: 96.875, Loss: 0.054321  [26944/57600]\n",
      "Accuracy: 95.3125, Loss: 0.086194  [27584/57600]\n",
      "Accuracy: 90.625, Loss: 0.163554  [28224/57600]\n",
      "Accuracy: 84.375, Loss: 0.229655  [28864/57600]\n",
      "Accuracy: 90.625, Loss: 0.119930  [29504/57600]\n",
      "Accuracy: 85.9375, Loss: 0.183434  [30144/57600]\n",
      "Accuracy: 90.625, Loss: 0.230163  [30784/57600]\n",
      "Accuracy: 84.375, Loss: 0.180796  [31424/57600]\n",
      "Accuracy: 87.5, Loss: 0.121836  [32064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113764  [32704/57600]\n",
      "Accuracy: 96.875, Loss: 0.069156  [33344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.163704  [33984/57600]\n",
      "Accuracy: 87.5, Loss: 0.135907  [34624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.126584  [35264/57600]\n",
      "Accuracy: 90.625, Loss: 0.087191  [35904/57600]\n",
      "Accuracy: 96.875, Loss: 0.073519  [36544/57600]\n",
      "Accuracy: 87.5, Loss: 0.158654  [37184/57600]\n",
      "Accuracy: 87.5, Loss: 0.161537  [37824/57600]\n",
      "Accuracy: 87.5, Loss: 0.167227  [38464/57600]\n",
      "Accuracy: 95.3125, Loss: 0.076404  [39104/57600]\n",
      "Accuracy: 85.9375, Loss: 0.190930  [39744/57600]\n",
      "Accuracy: 87.5, Loss: 0.148041  [40384/57600]\n",
      "Accuracy: 87.5, Loss: 0.165853  [41024/57600]\n",
      "Accuracy: 90.625, Loss: 0.130789  [41664/57600]\n",
      "Accuracy: 87.5, Loss: 0.126032  [42304/57600]\n",
      "Accuracy: 92.1875, Loss: 0.130896  [42944/57600]\n",
      "Accuracy: 89.0625, Loss: 0.113497  [43584/57600]\n",
      "Accuracy: 89.0625, Loss: 0.137927  [44224/57600]\n",
      "Accuracy: 89.0625, Loss: 0.106267  [44864/57600]\n",
      "Accuracy: 85.9375, Loss: 0.128070  [45504/57600]\n",
      "Accuracy: 95.3125, Loss: 0.088288  [46144/57600]\n",
      "Accuracy: 84.375, Loss: 0.115263  [46784/57600]\n",
      "Accuracy: 82.8125, Loss: 0.242422  [47424/57600]\n",
      "Accuracy: 87.5, Loss: 0.148601  [48064/57600]\n",
      "Accuracy: 90.625, Loss: 0.106698  [48704/57600]\n",
      "Accuracy: 85.9375, Loss: 0.173779  [49344/57600]\n",
      "Accuracy: 90.625, Loss: 0.122012  [49984/57600]\n",
      "Accuracy: 93.75, Loss: 0.090119  [50624/57600]\n",
      "Accuracy: 90.625, Loss: 0.101404  [51264/57600]\n",
      "Accuracy: 95.3125, Loss: 0.077785  [51904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.130241  [52544/57600]\n",
      "Accuracy: 89.0625, Loss: 0.164516  [53184/57600]\n",
      "Accuracy: 93.75, Loss: 0.084097  [53824/57600]\n",
      "Accuracy: 93.75, Loss: 0.096358  [54464/57600]\n",
      "Accuracy: 82.8125, Loss: 0.313605  [55104/57600]\n",
      "Accuracy: 89.0625, Loss: 0.163195  [55744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.142477  [56384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.128812  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.129625 \n",
      "           Precision: 0.882, Recall: 0.916\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Accuracy: 87.5, Loss: 0.134122  [   64/57600]\n",
      "Accuracy: 89.0625, Loss: 0.153253  [  704/57600]\n",
      "Accuracy: 90.625, Loss: 0.144417  [ 1344/57600]\n",
      "Accuracy: 93.75, Loss: 0.084843  [ 1984/57600]\n",
      "Accuracy: 93.75, Loss: 0.090581  [ 2624/57600]\n",
      "Accuracy: 89.0625, Loss: 0.118154  [ 3264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.145154  [ 3904/57600]\n",
      "Accuracy: 82.8125, Loss: 0.228422  [ 4544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.101141  [ 5184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.095060  [ 5824/57600]\n",
      "Accuracy: 90.625, Loss: 0.108034  [ 6464/57600]\n",
      "Accuracy: 84.375, Loss: 0.179401  [ 7104/57600]\n",
      "Accuracy: 93.75, Loss: 0.093064  [ 7744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.140018  [ 8384/57600]\n",
      "Accuracy: 82.8125, Loss: 0.180333  [ 9024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.121602  [ 9664/57600]\n",
      "Accuracy: 87.5, Loss: 0.115269  [10304/57600]\n",
      "Accuracy: 84.375, Loss: 0.167753  [10944/57600]\n",
      "Accuracy: 93.75, Loss: 0.090934  [11584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.077199  [12224/57600]\n",
      "Accuracy: 93.75, Loss: 0.108160  [12864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.136482  [13504/57600]\n",
      "Accuracy: 93.75, Loss: 0.086406  [14144/57600]\n",
      "Accuracy: 93.75, Loss: 0.123556  [14784/57600]\n",
      "Accuracy: 79.6875, Loss: 0.184769  [15424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086490  [16064/57600]\n",
      "Accuracy: 90.625, Loss: 0.128811  [16704/57600]\n",
      "Accuracy: 93.75, Loss: 0.095291  [17344/57600]\n",
      "Accuracy: 85.9375, Loss: 0.149028  [17984/57600]\n",
      "Accuracy: 90.625, Loss: 0.122650  [18624/57600]\n",
      "Accuracy: 85.9375, Loss: 0.185948  [19264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.131615  [19904/57600]\n",
      "Accuracy: 90.625, Loss: 0.114503  [20544/57600]\n",
      "Accuracy: 87.5, Loss: 0.179447  [21184/57600]\n",
      "Accuracy: 85.9375, Loss: 0.126342  [21824/57600]\n",
      "Accuracy: 85.9375, Loss: 0.142676  [22464/57600]\n",
      "Accuracy: 90.625, Loss: 0.090835  [23104/57600]\n",
      "Accuracy: 87.5, Loss: 0.117815  [23744/57600]\n",
      "Accuracy: 84.375, Loss: 0.178485  [24384/57600]\n",
      "Accuracy: 90.625, Loss: 0.148229  [25024/57600]\n",
      "Accuracy: 90.625, Loss: 0.126487  [25664/57600]\n",
      "Accuracy: 84.375, Loss: 0.177795  [26304/57600]\n",
      "Accuracy: 81.25, Loss: 0.175038  [26944/57600]\n",
      "Accuracy: 82.8125, Loss: 0.150194  [27584/57600]\n",
      "Accuracy: 87.5, Loss: 0.114873  [28224/57600]\n",
      "Accuracy: 85.9375, Loss: 0.119351  [28864/57600]\n",
      "Accuracy: 87.5, Loss: 0.139475  [29504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.136710  [30144/57600]\n",
      "Accuracy: 85.9375, Loss: 0.181752  [30784/57600]\n",
      "Accuracy: 79.6875, Loss: 0.143953  [31424/57600]\n",
      "Accuracy: 87.5, Loss: 0.166280  [32064/57600]\n",
      "Accuracy: 85.9375, Loss: 0.187253  [32704/57600]\n",
      "Accuracy: 89.0625, Loss: 0.157871  [33344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.114319  [33984/57600]\n",
      "Accuracy: 82.8125, Loss: 0.183034  [34624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.103220  [35264/57600]\n",
      "Accuracy: 93.75, Loss: 0.077923  [35904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.122210  [36544/57600]\n",
      "Accuracy: 93.75, Loss: 0.066776  [37184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.151953  [37824/57600]\n",
      "Accuracy: 89.0625, Loss: 0.144060  [38464/57600]\n",
      "Accuracy: 90.625, Loss: 0.132806  [39104/57600]\n",
      "Accuracy: 98.4375, Loss: 0.065412  [39744/57600]\n",
      "Accuracy: 90.625, Loss: 0.119117  [40384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.146241  [41024/57600]\n",
      "Accuracy: 93.75, Loss: 0.087675  [41664/57600]\n",
      "Accuracy: 92.1875, Loss: 0.144424  [42304/57600]\n",
      "Accuracy: 93.75, Loss: 0.095464  [42944/57600]\n",
      "Accuracy: 93.75, Loss: 0.091471  [43584/57600]\n",
      "Accuracy: 84.375, Loss: 0.110770  [44224/57600]\n",
      "Accuracy: 95.3125, Loss: 0.065071  [44864/57600]\n",
      "Accuracy: 87.5, Loss: 0.155128  [45504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.145343  [46144/57600]\n",
      "Accuracy: 93.75, Loss: 0.092294  [46784/57600]\n",
      "Accuracy: 95.3125, Loss: 0.087870  [47424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.101066  [48064/57600]\n",
      "Accuracy: 96.875, Loss: 0.056886  [48704/57600]\n",
      "Accuracy: 96.875, Loss: 0.076292  [49344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.105393  [49984/57600]\n",
      "Accuracy: 93.75, Loss: 0.104721  [50624/57600]\n",
      "Accuracy: 85.9375, Loss: 0.129698  [51264/57600]\n",
      "Accuracy: 76.5625, Loss: 0.248794  [51904/57600]\n",
      "Accuracy: 90.625, Loss: 0.113493  [52544/57600]\n",
      "Accuracy: 93.75, Loss: 0.084032  [53184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.106982  [53824/57600]\n",
      "Accuracy: 82.8125, Loss: 0.144488  [54464/57600]\n",
      "Accuracy: 93.75, Loss: 0.073262  [55104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.119961  [55744/57600]\n",
      "Accuracy: 90.625, Loss: 0.151029  [56384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.076350  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.128575 \n",
      "           Precision: 0.878, Recall: 0.916\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Accuracy: 87.5, Loss: 0.181693  [   64/57600]\n",
      "Accuracy: 92.1875, Loss: 0.109567  [  704/57600]\n",
      "Accuracy: 85.9375, Loss: 0.154599  [ 1344/57600]\n",
      "Accuracy: 90.625, Loss: 0.158792  [ 1984/57600]\n",
      "Accuracy: 85.9375, Loss: 0.214819  [ 2624/57600]\n",
      "Accuracy: 96.875, Loss: 0.072962  [ 3264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.092244  [ 3904/57600]\n",
      "Accuracy: 90.625, Loss: 0.135654  [ 4544/57600]\n",
      "Accuracy: 87.5, Loss: 0.128444  [ 5184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.132621  [ 5824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.123244  [ 6464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.177075  [ 7104/57600]\n",
      "Accuracy: 98.4375, Loss: 0.063705  [ 7744/57600]\n",
      "Accuracy: 87.5, Loss: 0.172847  [ 8384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.153617  [ 9024/57600]\n",
      "Accuracy: 87.5, Loss: 0.113357  [ 9664/57600]\n",
      "Accuracy: 89.0625, Loss: 0.127669  [10304/57600]\n",
      "Accuracy: 84.375, Loss: 0.167320  [10944/57600]\n",
      "Accuracy: 90.625, Loss: 0.148540  [11584/57600]\n",
      "Accuracy: 96.875, Loss: 0.060592  [12224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.122656  [12864/57600]\n",
      "Accuracy: 90.625, Loss: 0.111636  [13504/57600]\n",
      "Accuracy: 85.9375, Loss: 0.146474  [14144/57600]\n",
      "Accuracy: 90.625, Loss: 0.096585  [14784/57600]\n",
      "Accuracy: 89.0625, Loss: 0.120269  [15424/57600]\n",
      "Accuracy: 90.625, Loss: 0.145839  [16064/57600]\n",
      "Accuracy: 93.75, Loss: 0.093626  [16704/57600]\n",
      "Accuracy: 87.5, Loss: 0.162860  [17344/57600]\n",
      "Accuracy: 95.3125, Loss: 0.081924  [17984/57600]\n",
      "Accuracy: 89.0625, Loss: 0.138148  [18624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.103752  [19264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.114792  [19904/57600]\n",
      "Accuracy: 93.75, Loss: 0.124584  [20544/57600]\n",
      "Accuracy: 90.625, Loss: 0.145811  [21184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.132380  [21824/57600]\n",
      "Accuracy: 90.625, Loss: 0.112489  [22464/57600]\n",
      "Accuracy: 90.625, Loss: 0.089487  [23104/57600]\n",
      "Accuracy: 89.0625, Loss: 0.108738  [23744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.088155  [24384/57600]\n",
      "Accuracy: 90.625, Loss: 0.125814  [25024/57600]\n",
      "Accuracy: 89.0625, Loss: 0.165730  [25664/57600]\n",
      "Accuracy: 93.75, Loss: 0.087401  [26304/57600]\n",
      "Accuracy: 92.1875, Loss: 0.094840  [26944/57600]\n",
      "Accuracy: 87.5, Loss: 0.188691  [27584/57600]\n",
      "Accuracy: 93.75, Loss: 0.096472  [28224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100241  [28864/57600]\n",
      "Accuracy: 92.1875, Loss: 0.111709  [29504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.095627  [30144/57600]\n",
      "Accuracy: 89.0625, Loss: 0.098919  [30784/57600]\n",
      "Accuracy: 89.0625, Loss: 0.159716  [31424/57600]\n",
      "Accuracy: 90.625, Loss: 0.122661  [32064/57600]\n",
      "Accuracy: 85.9375, Loss: 0.141093  [32704/57600]\n",
      "Accuracy: 87.5, Loss: 0.139954  [33344/57600]\n",
      "Accuracy: 95.3125, Loss: 0.090898  [33984/57600]\n",
      "Accuracy: 95.3125, Loss: 0.090867  [34624/57600]\n",
      "Accuracy: 87.5, Loss: 0.115434  [35264/57600]\n",
      "Accuracy: 93.75, Loss: 0.092728  [35904/57600]\n",
      "Accuracy: 82.8125, Loss: 0.264715  [36544/57600]\n",
      "Accuracy: 93.75, Loss: 0.121721  [37184/57600]\n",
      "Accuracy: 90.625, Loss: 0.113459  [37824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.103393  [38464/57600]\n",
      "Accuracy: 90.625, Loss: 0.129375  [39104/57600]\n",
      "Accuracy: 87.5, Loss: 0.188924  [39744/57600]\n",
      "Accuracy: 87.5, Loss: 0.190372  [40384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.121350  [41024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.105967  [41664/57600]\n",
      "Accuracy: 96.875, Loss: 0.081123  [42304/57600]\n",
      "Accuracy: 89.0625, Loss: 0.138253  [42944/57600]\n",
      "Accuracy: 95.3125, Loss: 0.086654  [43584/57600]\n",
      "Accuracy: 90.625, Loss: 0.110842  [44224/57600]\n",
      "Accuracy: 90.625, Loss: 0.097190  [44864/57600]\n",
      "Accuracy: 90.625, Loss: 0.106381  [45504/57600]\n",
      "Accuracy: 87.5, Loss: 0.145187  [46144/57600]\n",
      "Accuracy: 85.9375, Loss: 0.168737  [46784/57600]\n",
      "Accuracy: 90.625, Loss: 0.123736  [47424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.089532  [48064/57600]\n",
      "Accuracy: 85.9375, Loss: 0.139637  [48704/57600]\n",
      "Accuracy: 90.625, Loss: 0.131113  [49344/57600]\n",
      "Accuracy: 93.75, Loss: 0.107161  [49984/57600]\n",
      "Accuracy: 89.0625, Loss: 0.150066  [50624/57600]\n",
      "Accuracy: 85.9375, Loss: 0.155208  [51264/57600]\n",
      "Accuracy: 93.75, Loss: 0.110913  [51904/57600]\n",
      "Accuracy: 90.625, Loss: 0.087809  [52544/57600]\n",
      "Accuracy: 87.5, Loss: 0.131728  [53184/57600]\n",
      "Accuracy: 95.3125, Loss: 0.087584  [53824/57600]\n",
      "Accuracy: 90.625, Loss: 0.087237  [54464/57600]\n",
      "Accuracy: 93.75, Loss: 0.107550  [55104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.120758  [55744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.146491  [56384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.071577  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.352846 \n",
      "           Precision: 0.587, Recall: 0.998\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Accuracy: 93.75, Loss: 0.089197  [   64/57600]\n",
      "Accuracy: 89.0625, Loss: 0.119122  [  704/57600]\n",
      "Accuracy: 90.625, Loss: 0.132310  [ 1344/57600]\n",
      "Accuracy: 87.5, Loss: 0.166678  [ 1984/57600]\n",
      "Accuracy: 90.625, Loss: 0.144742  [ 2624/57600]\n",
      "Accuracy: 93.75, Loss: 0.134139  [ 3264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.157679  [ 3904/57600]\n",
      "Accuracy: 90.625, Loss: 0.097227  [ 4544/57600]\n",
      "Accuracy: 90.625, Loss: 0.099149  [ 5184/57600]\n",
      "Accuracy: 93.75, Loss: 0.084987  [ 5824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086396  [ 6464/57600]\n",
      "Accuracy: 85.9375, Loss: 0.147768  [ 7104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.108007  [ 7744/57600]\n",
      "Accuracy: 84.375, Loss: 0.178992  [ 8384/57600]\n",
      "Accuracy: 87.5, Loss: 0.166479  [ 9024/57600]\n",
      "Accuracy: 89.0625, Loss: 0.106206  [ 9664/57600]\n",
      "Accuracy: 89.0625, Loss: 0.155721  [10304/57600]\n",
      "Accuracy: 85.9375, Loss: 0.135310  [10944/57600]\n",
      "Accuracy: 93.75, Loss: 0.073393  [11584/57600]\n",
      "Accuracy: 87.5, Loss: 0.156269  [12224/57600]\n",
      "Accuracy: 93.75, Loss: 0.098660  [12864/57600]\n",
      "Accuracy: 87.5, Loss: 0.163977  [13504/57600]\n",
      "Accuracy: 85.9375, Loss: 0.152880  [14144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.123977  [14784/57600]\n",
      "Accuracy: 90.625, Loss: 0.093788  [15424/57600]\n",
      "Accuracy: 87.5, Loss: 0.131959  [16064/57600]\n",
      "Accuracy: 87.5, Loss: 0.133213  [16704/57600]\n",
      "Accuracy: 84.375, Loss: 0.159135  [17344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.090342  [17984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.080058  [18624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.080037  [19264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.122889  [19904/57600]\n",
      "Accuracy: 93.75, Loss: 0.079588  [20544/57600]\n",
      "Accuracy: 96.875, Loss: 0.057556  [21184/57600]\n",
      "Accuracy: 81.25, Loss: 0.181330  [21824/57600]\n",
      "Accuracy: 89.0625, Loss: 0.107573  [22464/57600]\n",
      "Accuracy: 93.75, Loss: 0.116500  [23104/57600]\n",
      "Accuracy: 93.75, Loss: 0.088156  [23744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.147857  [24384/57600]\n",
      "Accuracy: 93.75, Loss: 0.078551  [25024/57600]\n",
      "Accuracy: 89.0625, Loss: 0.137635  [25664/57600]\n",
      "Accuracy: 89.0625, Loss: 0.105617  [26304/57600]\n",
      "Accuracy: 84.375, Loss: 0.168102  [26944/57600]\n",
      "Accuracy: 81.25, Loss: 0.264447  [27584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086729  [28224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113915  [28864/57600]\n",
      "Accuracy: 90.625, Loss: 0.129672  [29504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.098019  [30144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113407  [30784/57600]\n",
      "Accuracy: 95.3125, Loss: 0.080756  [31424/57600]\n",
      "Accuracy: 89.0625, Loss: 0.131966  [32064/57600]\n",
      "Accuracy: 84.375, Loss: 0.179742  [32704/57600]\n",
      "Accuracy: 89.0625, Loss: 0.132938  [33344/57600]\n",
      "Accuracy: 87.5, Loss: 0.189461  [33984/57600]\n",
      "Accuracy: 87.5, Loss: 0.152166  [34624/57600]\n",
      "Accuracy: 90.625, Loss: 0.128017  [35264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.118645  [35904/57600]\n",
      "Accuracy: 98.4375, Loss: 0.058143  [36544/57600]\n",
      "Accuracy: 87.5, Loss: 0.155031  [37184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.135999  [37824/57600]\n",
      "Accuracy: 98.4375, Loss: 0.054575  [38464/57600]\n",
      "Accuracy: 90.625, Loss: 0.136202  [39104/57600]\n",
      "Accuracy: 85.9375, Loss: 0.167648  [39744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.122751  [40384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.104434  [41024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.081175  [41664/57600]\n",
      "Accuracy: 96.875, Loss: 0.077921  [42304/57600]\n",
      "Accuracy: 92.1875, Loss: 0.137056  [42944/57600]\n",
      "Accuracy: 90.625, Loss: 0.084580  [43584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.080629  [44224/57600]\n",
      "Accuracy: 85.9375, Loss: 0.137207  [44864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.139930  [45504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.121912  [46144/57600]\n",
      "Accuracy: 84.375, Loss: 0.157057  [46784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.135852  [47424/57600]\n",
      "Accuracy: 95.3125, Loss: 0.061097  [48064/57600]\n",
      "Accuracy: 87.5, Loss: 0.148484  [48704/57600]\n",
      "Accuracy: 93.75, Loss: 0.111923  [49344/57600]\n",
      "Accuracy: 87.5, Loss: 0.130567  [49984/57600]\n",
      "Accuracy: 90.625, Loss: 0.125752  [50624/57600]\n",
      "Accuracy: 90.625, Loss: 0.137800  [51264/57600]\n",
      "Accuracy: 95.3125, Loss: 0.076558  [51904/57600]\n",
      "Accuracy: 90.625, Loss: 0.111153  [52544/57600]\n",
      "Accuracy: 90.625, Loss: 0.179720  [53184/57600]\n",
      "Accuracy: 93.75, Loss: 0.135918  [53824/57600]\n",
      "Accuracy: 90.625, Loss: 0.095212  [54464/57600]\n",
      "Accuracy: 87.5, Loss: 0.129267  [55104/57600]\n",
      "Accuracy: 87.5, Loss: 0.137050  [55744/57600]\n",
      "Accuracy: 87.5, Loss: 0.115468  [56384/57600]\n",
      "Accuracy: 87.5, Loss: 0.159969  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 1.090541 \n",
      "           Precision: 0.938, Recall: 0.341\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Accuracy: 93.75, Loss: 0.142212  [   64/57600]\n",
      "Accuracy: 92.1875, Loss: 0.124098  [  704/57600]\n",
      "Accuracy: 93.75, Loss: 0.102759  [ 1344/57600]\n",
      "Accuracy: 93.75, Loss: 0.129287  [ 1984/57600]\n",
      "Accuracy: 93.75, Loss: 0.084393  [ 2624/57600]\n",
      "Accuracy: 90.625, Loss: 0.128262  [ 3264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.233608  [ 3904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.110923  [ 4544/57600]\n",
      "Accuracy: 96.875, Loss: 0.059048  [ 5184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.118000  [ 5824/57600]\n",
      "Accuracy: 89.0625, Loss: 0.140630  [ 6464/57600]\n",
      "Accuracy: 90.625, Loss: 0.136267  [ 7104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.065778  [ 7744/57600]\n",
      "Accuracy: 85.9375, Loss: 0.157861  [ 8384/57600]\n",
      "Accuracy: 90.625, Loss: 0.124539  [ 9024/57600]\n",
      "Accuracy: 84.375, Loss: 0.166935  [ 9664/57600]\n",
      "Accuracy: 85.9375, Loss: 0.147698  [10304/57600]\n",
      "Accuracy: 87.5, Loss: 0.160612  [10944/57600]\n",
      "Accuracy: 87.5, Loss: 0.124286  [11584/57600]\n",
      "Accuracy: 90.625, Loss: 0.142867  [12224/57600]\n",
      "Accuracy: 85.9375, Loss: 0.178589  [12864/57600]\n",
      "Accuracy: 92.1875, Loss: 0.096239  [13504/57600]\n",
      "Accuracy: 98.4375, Loss: 0.048900  [14144/57600]\n",
      "Accuracy: 98.4375, Loss: 0.037017  [14784/57600]\n",
      "Accuracy: 90.625, Loss: 0.110274  [15424/57600]\n",
      "Accuracy: 84.375, Loss: 0.170806  [16064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.099443  [16704/57600]\n",
      "Accuracy: 89.0625, Loss: 0.115891  [17344/57600]\n",
      "Accuracy: 90.625, Loss: 0.124961  [17984/57600]\n",
      "Accuracy: 90.625, Loss: 0.104031  [18624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.062968  [19264/57600]\n",
      "Accuracy: 96.875, Loss: 0.073189  [19904/57600]\n",
      "Accuracy: 93.75, Loss: 0.079203  [20544/57600]\n",
      "Accuracy: 90.625, Loss: 0.146201  [21184/57600]\n",
      "Accuracy: 96.875, Loss: 0.057265  [21824/57600]\n",
      "Accuracy: 90.625, Loss: 0.140844  [22464/57600]\n",
      "Accuracy: 93.75, Loss: 0.095033  [23104/57600]\n",
      "Accuracy: 93.75, Loss: 0.088262  [23744/57600]\n",
      "Accuracy: 96.875, Loss: 0.083314  [24384/57600]\n",
      "Accuracy: 87.5, Loss: 0.183018  [25024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.098840  [25664/57600]\n",
      "Accuracy: 90.625, Loss: 0.176422  [26304/57600]\n",
      "Accuracy: 95.3125, Loss: 0.057136  [26944/57600]\n",
      "Accuracy: 87.5, Loss: 0.152275  [27584/57600]\n",
      "Accuracy: 89.0625, Loss: 0.151138  [28224/57600]\n",
      "Accuracy: 90.625, Loss: 0.139306  [28864/57600]\n",
      "Accuracy: 96.875, Loss: 0.062258  [29504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.089314  [30144/57600]\n",
      "Accuracy: 89.0625, Loss: 0.114321  [30784/57600]\n",
      "Accuracy: 81.25, Loss: 0.200663  [31424/57600]\n",
      "Accuracy: 95.3125, Loss: 0.086412  [32064/57600]\n",
      "Accuracy: 87.5, Loss: 0.136969  [32704/57600]\n",
      "Accuracy: 87.5, Loss: 0.141854  [33344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.108281  [33984/57600]\n",
      "Accuracy: 90.625, Loss: 0.102190  [34624/57600]\n",
      "Accuracy: 96.875, Loss: 0.060215  [35264/57600]\n",
      "Accuracy: 95.3125, Loss: 0.083384  [35904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.163077  [36544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.071687  [37184/57600]\n",
      "Accuracy: 90.625, Loss: 0.123036  [37824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.105084  [38464/57600]\n",
      "Accuracy: 90.625, Loss: 0.153040  [39104/57600]\n",
      "Accuracy: 89.0625, Loss: 0.141914  [39744/57600]\n",
      "Accuracy: 90.625, Loss: 0.098342  [40384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.065493  [41024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.119736  [41664/57600]\n",
      "Accuracy: 84.375, Loss: 0.150512  [42304/57600]\n",
      "Accuracy: 90.625, Loss: 0.106309  [42944/57600]\n",
      "Accuracy: 93.75, Loss: 0.086258  [43584/57600]\n",
      "Accuracy: 90.625, Loss: 0.108780  [44224/57600]\n",
      "Accuracy: 90.625, Loss: 0.101471  [44864/57600]\n",
      "Accuracy: 85.9375, Loss: 0.160788  [45504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.165127  [46144/57600]\n",
      "Accuracy: 82.8125, Loss: 0.155545  [46784/57600]\n",
      "Accuracy: 87.5, Loss: 0.167401  [47424/57600]\n",
      "Accuracy: 87.5, Loss: 0.150981  [48064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.075361  [48704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.090174  [49344/57600]\n",
      "Accuracy: 96.875, Loss: 0.068571  [49984/57600]\n",
      "Accuracy: 96.875, Loss: 0.067889  [50624/57600]\n",
      "Accuracy: 87.5, Loss: 0.125960  [51264/57600]\n",
      "Accuracy: 98.4375, Loss: 0.049431  [51904/57600]\n",
      "Accuracy: 95.3125, Loss: 0.075046  [52544/57600]\n",
      "Accuracy: 89.0625, Loss: 0.127166  [53184/57600]\n",
      "Accuracy: 84.375, Loss: 0.188271  [53824/57600]\n",
      "Accuracy: 85.9375, Loss: 0.172714  [54464/57600]\n",
      "Accuracy: 93.75, Loss: 0.072407  [55104/57600]\n",
      "Accuracy: 82.8125, Loss: 0.149869  [55744/57600]\n",
      "Accuracy: 87.5, Loss: 0.138963  [56384/57600]\n",
      "Accuracy: 93.75, Loss: 0.128607  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.125489 \n",
      "           Precision: 0.883, Recall: 0.940\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Accuracy: 95.3125, Loss: 0.065808  [   64/57600]\n",
      "Accuracy: 79.6875, Loss: 0.275487  [  704/57600]\n",
      "Accuracy: 93.75, Loss: 0.095344  [ 1344/57600]\n",
      "Accuracy: 85.9375, Loss: 0.137867  [ 1984/57600]\n",
      "Accuracy: 93.75, Loss: 0.085073  [ 2624/57600]\n",
      "Accuracy: 90.625, Loss: 0.163580  [ 3264/57600]\n",
      "Accuracy: 90.625, Loss: 0.140236  [ 3904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.146411  [ 4544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.061643  [ 5184/57600]\n",
      "Accuracy: 87.5, Loss: 0.131082  [ 5824/57600]\n",
      "Accuracy: 93.75, Loss: 0.081675  [ 6464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.071748  [ 7104/57600]\n",
      "Accuracy: 93.75, Loss: 0.075709  [ 7744/57600]\n",
      "Accuracy: 85.9375, Loss: 0.129853  [ 8384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.076854  [ 9024/57600]\n",
      "Accuracy: 90.625, Loss: 0.118044  [ 9664/57600]\n",
      "Accuracy: 90.625, Loss: 0.085606  [10304/57600]\n",
      "Accuracy: 89.0625, Loss: 0.108615  [10944/57600]\n",
      "Accuracy: 95.3125, Loss: 0.067678  [11584/57600]\n",
      "Accuracy: 85.9375, Loss: 0.142054  [12224/57600]\n",
      "Accuracy: 93.75, Loss: 0.095471  [12864/57600]\n",
      "Accuracy: 85.9375, Loss: 0.157713  [13504/57600]\n",
      "Accuracy: 95.3125, Loss: 0.064681  [14144/57600]\n",
      "Accuracy: 87.5, Loss: 0.171794  [14784/57600]\n",
      "Accuracy: 89.0625, Loss: 0.085936  [15424/57600]\n",
      "Accuracy: 90.625, Loss: 0.114114  [16064/57600]\n",
      "Accuracy: 98.4375, Loss: 0.059791  [16704/57600]\n",
      "Accuracy: 90.625, Loss: 0.102628  [17344/57600]\n",
      "Accuracy: 90.625, Loss: 0.096218  [17984/57600]\n",
      "Accuracy: 90.625, Loss: 0.127101  [18624/57600]\n",
      "Accuracy: 93.75, Loss: 0.090847  [19264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.097533  [19904/57600]\n",
      "Accuracy: 93.75, Loss: 0.106130  [20544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.079434  [21184/57600]\n",
      "Accuracy: 96.875, Loss: 0.061022  [21824/57600]\n",
      "Accuracy: 89.0625, Loss: 0.108981  [22464/57600]\n",
      "Accuracy: 90.625, Loss: 0.085546  [23104/57600]\n",
      "Accuracy: 87.5, Loss: 0.147481  [23744/57600]\n",
      "Accuracy: 93.75, Loss: 0.088036  [24384/57600]\n",
      "Accuracy: 85.9375, Loss: 0.178274  [25024/57600]\n",
      "Accuracy: 89.0625, Loss: 0.167111  [25664/57600]\n",
      "Accuracy: 85.9375, Loss: 0.161241  [26304/57600]\n",
      "Accuracy: 92.1875, Loss: 0.114916  [26944/57600]\n",
      "Accuracy: 87.5, Loss: 0.170633  [27584/57600]\n",
      "Accuracy: 89.0625, Loss: 0.110361  [28224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.096031  [28864/57600]\n",
      "Accuracy: 90.625, Loss: 0.085347  [29504/57600]\n",
      "Accuracy: 98.4375, Loss: 0.064930  [30144/57600]\n",
      "Accuracy: 95.3125, Loss: 0.074985  [30784/57600]\n",
      "Accuracy: 89.0625, Loss: 0.129176  [31424/57600]\n",
      "Accuracy: 89.0625, Loss: 0.104647  [32064/57600]\n",
      "Accuracy: 87.5, Loss: 0.139869  [32704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.114249  [33344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.116249  [33984/57600]\n",
      "Accuracy: 85.9375, Loss: 0.148338  [34624/57600]\n",
      "Accuracy: 90.625, Loss: 0.118714  [35264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100085  [35904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.108481  [36544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.064236  [37184/57600]\n",
      "Accuracy: 85.9375, Loss: 0.195876  [37824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.106763  [38464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.132309  [39104/57600]\n",
      "Accuracy: 90.625, Loss: 0.097703  [39744/57600]\n",
      "Accuracy: 93.75, Loss: 0.094716  [40384/57600]\n",
      "Accuracy: 90.625, Loss: 0.118014  [41024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.140162  [41664/57600]\n",
      "Accuracy: 92.1875, Loss: 0.109084  [42304/57600]\n",
      "Accuracy: 95.3125, Loss: 0.071390  [42944/57600]\n",
      "Accuracy: 87.5, Loss: 0.155843  [43584/57600]\n",
      "Accuracy: 82.8125, Loss: 0.277557  [44224/57600]\n",
      "Accuracy: 90.625, Loss: 0.109064  [44864/57600]\n",
      "Accuracy: 87.5, Loss: 0.151104  [45504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.117974  [46144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.095700  [46784/57600]\n",
      "Accuracy: 98.4375, Loss: 0.055098  [47424/57600]\n",
      "Accuracy: 95.3125, Loss: 0.072756  [48064/57600]\n",
      "Accuracy: 89.0625, Loss: 0.132930  [48704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.090814  [49344/57600]\n",
      "Accuracy: 93.75, Loss: 0.084469  [49984/57600]\n",
      "Accuracy: 90.625, Loss: 0.089246  [50624/57600]\n",
      "Accuracy: 87.5, Loss: 0.103619  [51264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.121287  [51904/57600]\n",
      "Accuracy: 93.75, Loss: 0.097161  [52544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.081179  [53184/57600]\n",
      "Accuracy: 93.75, Loss: 0.086300  [53824/57600]\n",
      "Accuracy: 93.75, Loss: 0.068651  [54464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.119492  [55104/57600]\n",
      "Accuracy: 90.625, Loss: 0.123945  [55744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.134514  [56384/57600]\n",
      "Accuracy: 90.625, Loss: 0.162054  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.120718 \n",
      "           Precision: 0.869, Recall: 0.949\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Accuracy: 90.625, Loss: 0.109223  [   64/57600]\n",
      "Accuracy: 90.625, Loss: 0.106145  [  704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.118900  [ 1344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.118452  [ 1984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.134073  [ 2624/57600]\n",
      "Accuracy: 90.625, Loss: 0.104874  [ 3264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.092421  [ 3904/57600]\n",
      "Accuracy: 98.4375, Loss: 0.049999  [ 4544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.112230  [ 5184/57600]\n",
      "Accuracy: 90.625, Loss: 0.098129  [ 5824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.106001  [ 6464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.134326  [ 7104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.096680  [ 7744/57600]\n",
      "Accuracy: 84.375, Loss: 0.124556  [ 8384/57600]\n",
      "Accuracy: 82.8125, Loss: 0.184856  [ 9024/57600]\n",
      "Accuracy: 87.5, Loss: 0.111361  [ 9664/57600]\n",
      "Accuracy: 93.75, Loss: 0.083279  [10304/57600]\n",
      "Accuracy: 90.625, Loss: 0.144566  [10944/57600]\n",
      "Accuracy: 90.625, Loss: 0.136472  [11584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.094541  [12224/57600]\n",
      "Accuracy: 89.0625, Loss: 0.129455  [12864/57600]\n",
      "Accuracy: 95.3125, Loss: 0.079221  [13504/57600]\n",
      "Accuracy: 96.875, Loss: 0.071738  [14144/57600]\n",
      "Accuracy: 89.0625, Loss: 0.103744  [14784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.115478  [15424/57600]\n",
      "Accuracy: 93.75, Loss: 0.115487  [16064/57600]\n",
      "Accuracy: 93.75, Loss: 0.111192  [16704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.107398  [17344/57600]\n",
      "Accuracy: 93.75, Loss: 0.109019  [17984/57600]\n",
      "Accuracy: 93.75, Loss: 0.104242  [18624/57600]\n",
      "Accuracy: 93.75, Loss: 0.058843  [19264/57600]\n",
      "Accuracy: 96.875, Loss: 0.073195  [19904/57600]\n",
      "Accuracy: 87.5, Loss: 0.137787  [20544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.103883  [21184/57600]\n",
      "Accuracy: 95.3125, Loss: 0.075634  [21824/57600]\n",
      "Accuracy: 93.75, Loss: 0.074932  [22464/57600]\n",
      "Accuracy: 93.75, Loss: 0.076055  [23104/57600]\n",
      "Accuracy: 90.625, Loss: 0.108591  [23744/57600]\n",
      "Accuracy: 87.5, Loss: 0.153626  [24384/57600]\n",
      "Accuracy: 96.875, Loss: 0.061366  [25024/57600]\n",
      "Accuracy: 95.3125, Loss: 0.065692  [25664/57600]\n",
      "Accuracy: 90.625, Loss: 0.104935  [26304/57600]\n",
      "Accuracy: 95.3125, Loss: 0.079386  [26944/57600]\n",
      "Accuracy: 92.1875, Loss: 0.102474  [27584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.094010  [28224/57600]\n",
      "Accuracy: 85.9375, Loss: 0.154465  [28864/57600]\n",
      "Accuracy: 96.875, Loss: 0.054306  [29504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.076405  [30144/57600]\n",
      "Accuracy: 93.75, Loss: 0.110920  [30784/57600]\n",
      "Accuracy: 95.3125, Loss: 0.039937  [31424/57600]\n",
      "Accuracy: 95.3125, Loss: 0.081835  [32064/57600]\n",
      "Accuracy: 85.9375, Loss: 0.159738  [32704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.088774  [33344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.157065  [33984/57600]\n",
      "Accuracy: 90.625, Loss: 0.161534  [34624/57600]\n",
      "Accuracy: 96.875, Loss: 0.054480  [35264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.075884  [35904/57600]\n",
      "Accuracy: 98.4375, Loss: 0.037386  [36544/57600]\n",
      "Accuracy: 96.875, Loss: 0.062959  [37184/57600]\n",
      "Accuracy: 90.625, Loss: 0.120907  [37824/57600]\n",
      "Accuracy: 90.625, Loss: 0.093521  [38464/57600]\n",
      "Accuracy: 90.625, Loss: 0.118418  [39104/57600]\n",
      "Accuracy: 93.75, Loss: 0.109823  [39744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.137591  [40384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.128915  [41024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.126436  [41664/57600]\n",
      "Accuracy: 89.0625, Loss: 0.114350  [42304/57600]\n",
      "Accuracy: 90.625, Loss: 0.125159  [42944/57600]\n",
      "Accuracy: 92.1875, Loss: 0.121200  [43584/57600]\n",
      "Accuracy: 93.75, Loss: 0.091134  [44224/57600]\n",
      "Accuracy: 90.625, Loss: 0.088186  [44864/57600]\n",
      "Accuracy: 98.4375, Loss: 0.056238  [45504/57600]\n",
      "Accuracy: 95.3125, Loss: 0.075776  [46144/57600]\n",
      "Accuracy: 93.75, Loss: 0.096110  [46784/57600]\n",
      "Accuracy: 84.375, Loss: 0.224642  [47424/57600]\n",
      "Accuracy: 85.9375, Loss: 0.195644  [48064/57600]\n",
      "Accuracy: 98.4375, Loss: 0.047179  [48704/57600]\n",
      "Accuracy: 93.75, Loss: 0.108167  [49344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.094880  [49984/57600]\n",
      "Accuracy: 93.75, Loss: 0.077966  [50624/57600]\n",
      "Accuracy: 85.9375, Loss: 0.118484  [51264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.089522  [51904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.109691  [52544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.122365  [53184/57600]\n",
      "Accuracy: 85.9375, Loss: 0.206821  [53824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.108262  [54464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.084862  [55104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.091385  [55744/57600]\n",
      "Accuracy: 84.375, Loss: 0.242390  [56384/57600]\n",
      "Accuracy: 90.625, Loss: 0.105937  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.113069 \n",
      "           Precision: 0.883, Recall: 0.954\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Accuracy: 82.8125, Loss: 0.159718  [   64/57600]\n",
      "Accuracy: 92.1875, Loss: 0.107462  [  704/57600]\n",
      "Accuracy: 87.5, Loss: 0.153334  [ 1344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.092786  [ 1984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.129878  [ 2624/57600]\n",
      "Accuracy: 93.75, Loss: 0.100481  [ 3264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.096025  [ 3904/57600]\n",
      "Accuracy: 93.75, Loss: 0.085702  [ 4544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.112342  [ 5184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.079968  [ 5824/57600]\n",
      "Accuracy: 89.0625, Loss: 0.144158  [ 6464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.158724  [ 7104/57600]\n",
      "Accuracy: 90.625, Loss: 0.114876  [ 7744/57600]\n",
      "Accuracy: 93.75, Loss: 0.090399  [ 8384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.139053  [ 9024/57600]\n",
      "Accuracy: 90.625, Loss: 0.111303  [ 9664/57600]\n",
      "Accuracy: 95.3125, Loss: 0.062083  [10304/57600]\n",
      "Accuracy: 96.875, Loss: 0.052461  [10944/57600]\n",
      "Accuracy: 89.0625, Loss: 0.156354  [11584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.090359  [12224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.105084  [12864/57600]\n",
      "Accuracy: 84.375, Loss: 0.146713  [13504/57600]\n",
      "Accuracy: 90.625, Loss: 0.096196  [14144/57600]\n",
      "Accuracy: 96.875, Loss: 0.056009  [14784/57600]\n",
      "Accuracy: 82.8125, Loss: 0.177331  [15424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.096177  [16064/57600]\n",
      "Accuracy: 87.5, Loss: 0.138563  [16704/57600]\n",
      "Accuracy: 96.875, Loss: 0.070239  [17344/57600]\n",
      "Accuracy: 90.625, Loss: 0.112177  [17984/57600]\n",
      "Accuracy: 93.75, Loss: 0.108847  [18624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.071832  [19264/57600]\n",
      "Accuracy: 90.625, Loss: 0.134905  [19904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.091367  [20544/57600]\n",
      "Accuracy: 84.375, Loss: 0.219144  [21184/57600]\n",
      "Accuracy: 93.75, Loss: 0.068048  [21824/57600]\n",
      "Accuracy: 87.5, Loss: 0.181678  [22464/57600]\n",
      "Accuracy: 87.5, Loss: 0.106967  [23104/57600]\n",
      "Accuracy: 89.0625, Loss: 0.131414  [23744/57600]\n",
      "Accuracy: 96.875, Loss: 0.055011  [24384/57600]\n",
      "Accuracy: 92.1875, Loss: 0.094913  [25024/57600]\n",
      "Accuracy: 82.8125, Loss: 0.192002  [25664/57600]\n",
      "Accuracy: 90.625, Loss: 0.098901  [26304/57600]\n",
      "Accuracy: 85.9375, Loss: 0.185348  [26944/57600]\n",
      "Accuracy: 95.3125, Loss: 0.062746  [27584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.111639  [28224/57600]\n",
      "Accuracy: 90.625, Loss: 0.100062  [28864/57600]\n",
      "Accuracy: 95.3125, Loss: 0.088111  [29504/57600]\n",
      "Accuracy: 93.75, Loss: 0.095135  [30144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.104809  [30784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.096841  [31424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.079889  [32064/57600]\n",
      "Accuracy: 90.625, Loss: 0.100751  [32704/57600]\n",
      "Accuracy: 100.0, Loss: 0.046325  [33344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.181580  [33984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.083611  [34624/57600]\n",
      "Accuracy: 98.4375, Loss: 0.031997  [35264/57600]\n",
      "Accuracy: 90.625, Loss: 0.154225  [35904/57600]\n",
      "Accuracy: 90.625, Loss: 0.124714  [36544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.139949  [37184/57600]\n",
      "Accuracy: 96.875, Loss: 0.065496  [37824/57600]\n",
      "Accuracy: 89.0625, Loss: 0.090336  [38464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.145480  [39104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.104818  [39744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.086169  [40384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.163151  [41024/57600]\n",
      "Accuracy: 93.75, Loss: 0.089550  [41664/57600]\n",
      "Accuracy: 89.0625, Loss: 0.139063  [42304/57600]\n",
      "Accuracy: 93.75, Loss: 0.098958  [42944/57600]\n",
      "Accuracy: 84.375, Loss: 0.209069  [43584/57600]\n",
      "Accuracy: 85.9375, Loss: 0.178368  [44224/57600]\n",
      "Accuracy: 96.875, Loss: 0.074828  [44864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.131909  [45504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.152894  [46144/57600]\n",
      "Accuracy: 96.875, Loss: 0.057454  [46784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086333  [47424/57600]\n",
      "Accuracy: 87.5, Loss: 0.165675  [48064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.075593  [48704/57600]\n",
      "Accuracy: 93.75, Loss: 0.113158  [49344/57600]\n",
      "Accuracy: 95.3125, Loss: 0.080214  [49984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.099970  [50624/57600]\n",
      "Accuracy: 90.625, Loss: 0.098022  [51264/57600]\n",
      "Accuracy: 90.625, Loss: 0.098559  [51904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.111250  [52544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.155802  [53184/57600]\n",
      "Accuracy: 95.3125, Loss: 0.074391  [53824/57600]\n",
      "Accuracy: 93.75, Loss: 0.074305  [54464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.175771  [55104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.076005  [55744/57600]\n",
      "Accuracy: 87.5, Loss: 0.126418  [56384/57600]\n",
      "Accuracy: 90.625, Loss: 0.133431  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.104748 \n",
      "           Precision: 0.885, Recall: 0.970\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Accuracy: 95.3125, Loss: 0.073415  [   64/57600]\n",
      "Accuracy: 87.5, Loss: 0.133713  [  704/57600]\n",
      "Accuracy: 93.75, Loss: 0.095934  [ 1344/57600]\n",
      "Accuracy: 90.625, Loss: 0.137447  [ 1984/57600]\n",
      "Accuracy: 89.0625, Loss: 0.125537  [ 2624/57600]\n",
      "Accuracy: 90.625, Loss: 0.102824  [ 3264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.099981  [ 3904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.103094  [ 4544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.145910  [ 5184/57600]\n",
      "Accuracy: 93.75, Loss: 0.081382  [ 5824/57600]\n",
      "Accuracy: 90.625, Loss: 0.099786  [ 6464/57600]\n",
      "Accuracy: 90.625, Loss: 0.101422  [ 7104/57600]\n",
      "Accuracy: 82.8125, Loss: 0.164599  [ 7744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.096165  [ 8384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.121920  [ 9024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.108914  [ 9664/57600]\n",
      "Accuracy: 90.625, Loss: 0.104973  [10304/57600]\n",
      "Accuracy: 85.9375, Loss: 0.122884  [10944/57600]\n",
      "Accuracy: 96.875, Loss: 0.065390  [11584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.071471  [12224/57600]\n",
      "Accuracy: 84.375, Loss: 0.127610  [12864/57600]\n",
      "Accuracy: 95.3125, Loss: 0.092181  [13504/57600]\n",
      "Accuracy: 96.875, Loss: 0.065724  [14144/57600]\n",
      "Accuracy: 87.5, Loss: 0.142807  [14784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.099881  [15424/57600]\n",
      "Accuracy: 93.75, Loss: 0.066313  [16064/57600]\n",
      "Accuracy: 90.625, Loss: 0.158395  [16704/57600]\n",
      "Accuracy: 98.4375, Loss: 0.042149  [17344/57600]\n",
      "Accuracy: 85.9375, Loss: 0.164756  [17984/57600]\n",
      "Accuracy: 95.3125, Loss: 0.063564  [18624/57600]\n",
      "Accuracy: 96.875, Loss: 0.046212  [19264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.215353  [19904/57600]\n",
      "Accuracy: 95.3125, Loss: 0.082004  [20544/57600]\n",
      "Accuracy: 85.9375, Loss: 0.170869  [21184/57600]\n",
      "Accuracy: 100.0, Loss: 0.042735  [21824/57600]\n",
      "Accuracy: 87.5, Loss: 0.152650  [22464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.117486  [23104/57600]\n",
      "Accuracy: 93.75, Loss: 0.100234  [23744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.081505  [24384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.078203  [25024/57600]\n",
      "Accuracy: 92.1875, Loss: 0.114496  [25664/57600]\n",
      "Accuracy: 90.625, Loss: 0.145749  [26304/57600]\n",
      "Accuracy: 93.75, Loss: 0.108693  [26944/57600]\n",
      "Accuracy: 95.3125, Loss: 0.055484  [27584/57600]\n",
      "Accuracy: 93.75, Loss: 0.094436  [28224/57600]\n",
      "Accuracy: 90.625, Loss: 0.129656  [28864/57600]\n",
      "Accuracy: 95.3125, Loss: 0.066835  [29504/57600]\n",
      "Accuracy: 95.3125, Loss: 0.084513  [30144/57600]\n",
      "Accuracy: 89.0625, Loss: 0.116976  [30784/57600]\n",
      "Accuracy: 82.8125, Loss: 0.172837  [31424/57600]\n",
      "Accuracy: 93.75, Loss: 0.089093  [32064/57600]\n",
      "Accuracy: 93.75, Loss: 0.091250  [32704/57600]\n",
      "Accuracy: 84.375, Loss: 0.188366  [33344/57600]\n",
      "Accuracy: 87.5, Loss: 0.132535  [33984/57600]\n",
      "Accuracy: 95.3125, Loss: 0.087936  [34624/57600]\n",
      "Accuracy: 89.0625, Loss: 0.125226  [35264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.123045  [35904/57600]\n",
      "Accuracy: 87.5, Loss: 0.175698  [36544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.092539  [37184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.116510  [37824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100354  [38464/57600]\n",
      "Accuracy: 95.3125, Loss: 0.095272  [39104/57600]\n",
      "Accuracy: 90.625, Loss: 0.095702  [39744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.062053  [40384/57600]\n",
      "Accuracy: 96.875, Loss: 0.063326  [41024/57600]\n",
      "Accuracy: 90.625, Loss: 0.099974  [41664/57600]\n",
      "Accuracy: 98.4375, Loss: 0.060257  [42304/57600]\n",
      "Accuracy: 95.3125, Loss: 0.074338  [42944/57600]\n",
      "Accuracy: 89.0625, Loss: 0.146514  [43584/57600]\n",
      "Accuracy: 90.625, Loss: 0.145303  [44224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.093574  [44864/57600]\n",
      "Accuracy: 90.625, Loss: 0.135591  [45504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.104388  [46144/57600]\n",
      "Accuracy: 90.625, Loss: 0.151718  [46784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.096621  [47424/57600]\n",
      "Accuracy: 87.5, Loss: 0.126483  [48064/57600]\n",
      "Accuracy: 89.0625, Loss: 0.156791  [48704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.094670  [49344/57600]\n",
      "Accuracy: 93.75, Loss: 0.115833  [49984/57600]\n",
      "Accuracy: 85.9375, Loss: 0.165961  [50624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.072017  [51264/57600]\n",
      "Accuracy: 96.875, Loss: 0.044181  [51904/57600]\n",
      "Accuracy: 90.625, Loss: 0.123093  [52544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.104506  [53184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.088262  [53824/57600]\n",
      "Accuracy: 93.75, Loss: 0.095798  [54464/57600]\n",
      "Accuracy: 90.625, Loss: 0.087088  [55104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.115961  [55744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.073707  [56384/57600]\n",
      "Accuracy: 90.625, Loss: 0.104649  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.108289 \n",
      "           Precision: 0.877, Recall: 0.966\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Accuracy: 89.0625, Loss: 0.114223  [   64/57600]\n",
      "Accuracy: 95.3125, Loss: 0.067538  [  704/57600]\n",
      "Accuracy: 89.0625, Loss: 0.120509  [ 1344/57600]\n",
      "Accuracy: 96.875, Loss: 0.058729  [ 1984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.080375  [ 2624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.108430  [ 3264/57600]\n",
      "Accuracy: 90.625, Loss: 0.110545  [ 3904/57600]\n",
      "Accuracy: 93.75, Loss: 0.103414  [ 4544/57600]\n",
      "Accuracy: 90.625, Loss: 0.111565  [ 5184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.160879  [ 5824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.139611  [ 6464/57600]\n",
      "Accuracy: 93.75, Loss: 0.097603  [ 7104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.095672  [ 7744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086062  [ 8384/57600]\n",
      "Accuracy: 79.6875, Loss: 0.224261  [ 9024/57600]\n",
      "Accuracy: 95.3125, Loss: 0.072184  [ 9664/57600]\n",
      "Accuracy: 89.0625, Loss: 0.162016  [10304/57600]\n",
      "Accuracy: 98.4375, Loss: 0.048254  [10944/57600]\n",
      "Accuracy: 92.1875, Loss: 0.078628  [11584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.061679  [12224/57600]\n",
      "Accuracy: 89.0625, Loss: 0.156779  [12864/57600]\n",
      "Accuracy: 92.1875, Loss: 0.105848  [13504/57600]\n",
      "Accuracy: 93.75, Loss: 0.075070  [14144/57600]\n",
      "Accuracy: 89.0625, Loss: 0.119257  [14784/57600]\n",
      "Accuracy: 93.75, Loss: 0.083800  [15424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.081822  [16064/57600]\n",
      "Accuracy: 95.3125, Loss: 0.094974  [16704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.152118  [17344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.126162  [17984/57600]\n",
      "Accuracy: 96.875, Loss: 0.048460  [18624/57600]\n",
      "Accuracy: 93.75, Loss: 0.081291  [19264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.141518  [19904/57600]\n",
      "Accuracy: 85.9375, Loss: 0.145926  [20544/57600]\n",
      "Accuracy: 93.75, Loss: 0.078524  [21184/57600]\n",
      "Accuracy: 93.75, Loss: 0.096977  [21824/57600]\n",
      "Accuracy: 85.9375, Loss: 0.195890  [22464/57600]\n",
      "Accuracy: 84.375, Loss: 0.172149  [23104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.049707  [23744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.089366  [24384/57600]\n",
      "Accuracy: 92.1875, Loss: 0.084973  [25024/57600]\n",
      "Accuracy: 90.625, Loss: 0.104225  [25664/57600]\n",
      "Accuracy: 96.875, Loss: 0.057951  [26304/57600]\n",
      "Accuracy: 87.5, Loss: 0.125161  [26944/57600]\n",
      "Accuracy: 90.625, Loss: 0.114448  [27584/57600]\n",
      "Accuracy: 96.875, Loss: 0.087661  [28224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.094834  [28864/57600]\n",
      "Accuracy: 95.3125, Loss: 0.092449  [29504/57600]\n",
      "Accuracy: 96.875, Loss: 0.060136  [30144/57600]\n",
      "Accuracy: 90.625, Loss: 0.142092  [30784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.130481  [31424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.107560  [32064/57600]\n",
      "Accuracy: 98.4375, Loss: 0.053207  [32704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.108962  [33344/57600]\n",
      "Accuracy: 85.9375, Loss: 0.133607  [33984/57600]\n",
      "Accuracy: 95.3125, Loss: 0.088756  [34624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.091510  [35264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.119297  [35904/57600]\n",
      "Accuracy: 87.5, Loss: 0.146371  [36544/57600]\n",
      "Accuracy: 87.5, Loss: 0.163470  [37184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.085833  [37824/57600]\n",
      "Accuracy: 90.625, Loss: 0.131971  [38464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.074077  [39104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.060025  [39744/57600]\n",
      "Accuracy: 93.75, Loss: 0.110760  [40384/57600]\n",
      "Accuracy: 93.75, Loss: 0.080823  [41024/57600]\n",
      "Accuracy: 93.75, Loss: 0.073308  [41664/57600]\n",
      "Accuracy: 93.75, Loss: 0.110731  [42304/57600]\n",
      "Accuracy: 93.75, Loss: 0.072955  [42944/57600]\n",
      "Accuracy: 93.75, Loss: 0.110760  [43584/57600]\n",
      "Accuracy: 93.75, Loss: 0.099025  [44224/57600]\n",
      "Accuracy: 96.875, Loss: 0.064423  [44864/57600]\n",
      "Accuracy: 92.1875, Loss: 0.089738  [45504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.111483  [46144/57600]\n",
      "Accuracy: 96.875, Loss: 0.041885  [46784/57600]\n",
      "Accuracy: 98.4375, Loss: 0.037764  [47424/57600]\n",
      "Accuracy: 90.625, Loss: 0.137847  [48064/57600]\n",
      "Accuracy: 96.875, Loss: 0.075317  [48704/57600]\n",
      "Accuracy: 87.5, Loss: 0.143907  [49344/57600]\n",
      "Accuracy: 96.875, Loss: 0.061802  [49984/57600]\n",
      "Accuracy: 90.625, Loss: 0.135005  [50624/57600]\n",
      "Accuracy: 87.5, Loss: 0.126000  [51264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.114426  [51904/57600]\n",
      "Accuracy: 90.625, Loss: 0.103850  [52544/57600]\n",
      "Accuracy: 96.875, Loss: 0.065228  [53184/57600]\n",
      "Accuracy: 82.8125, Loss: 0.119010  [53824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.076615  [54464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.092562  [55104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113026  [55744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.078202  [56384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.064420  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.097112 \n",
      "           Precision: 0.895, Recall: 0.970\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Accuracy: 85.9375, Loss: 0.152923  [   64/57600]\n",
      "Accuracy: 95.3125, Loss: 0.069235  [  704/57600]\n",
      "Accuracy: 90.625, Loss: 0.103580  [ 1344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.122889  [ 1984/57600]\n",
      "Accuracy: 90.625, Loss: 0.102464  [ 2624/57600]\n",
      "Accuracy: 89.0625, Loss: 0.121262  [ 3264/57600]\n",
      "Accuracy: 98.4375, Loss: 0.052222  [ 3904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.106979  [ 4544/57600]\n",
      "Accuracy: 90.625, Loss: 0.142463  [ 5184/57600]\n",
      "Accuracy: 93.75, Loss: 0.066870  [ 5824/57600]\n",
      "Accuracy: 93.75, Loss: 0.132703  [ 6464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.160648  [ 7104/57600]\n",
      "Accuracy: 82.8125, Loss: 0.234136  [ 7744/57600]\n",
      "Accuracy: 82.8125, Loss: 0.158758  [ 8384/57600]\n",
      "Accuracy: 93.75, Loss: 0.088263  [ 9024/57600]\n",
      "Accuracy: 90.625, Loss: 0.079207  [ 9664/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086362  [10304/57600]\n",
      "Accuracy: 87.5, Loss: 0.139950  [10944/57600]\n",
      "Accuracy: 93.75, Loss: 0.093932  [11584/57600]\n",
      "Accuracy: 98.4375, Loss: 0.045397  [12224/57600]\n",
      "Accuracy: 87.5, Loss: 0.135464  [12864/57600]\n",
      "Accuracy: 95.3125, Loss: 0.067762  [13504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.101250  [14144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.073465  [14784/57600]\n",
      "Accuracy: 89.0625, Loss: 0.138074  [15424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100394  [16064/57600]\n",
      "Accuracy: 95.3125, Loss: 0.039514  [16704/57600]\n",
      "Accuracy: 93.75, Loss: 0.102321  [17344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.102027  [17984/57600]\n",
      "Accuracy: 87.5, Loss: 0.141891  [18624/57600]\n",
      "Accuracy: 93.75, Loss: 0.115914  [19264/57600]\n",
      "Accuracy: 90.625, Loss: 0.095363  [19904/57600]\n",
      "Accuracy: 90.625, Loss: 0.098814  [20544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.122186  [21184/57600]\n",
      "Accuracy: 87.5, Loss: 0.199654  [21824/57600]\n",
      "Accuracy: 95.3125, Loss: 0.074663  [22464/57600]\n",
      "Accuracy: 90.625, Loss: 0.088469  [23104/57600]\n",
      "Accuracy: 89.0625, Loss: 0.171388  [23744/57600]\n",
      "Accuracy: 90.625, Loss: 0.107261  [24384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.130480  [25024/57600]\n",
      "Accuracy: 90.625, Loss: 0.085364  [25664/57600]\n",
      "Accuracy: 89.0625, Loss: 0.114783  [26304/57600]\n",
      "Accuracy: 95.3125, Loss: 0.078499  [26944/57600]\n",
      "Accuracy: 92.1875, Loss: 0.077190  [27584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.088777  [28224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.091457  [28864/57600]\n",
      "Accuracy: 90.625, Loss: 0.104043  [29504/57600]\n",
      "Accuracy: 93.75, Loss: 0.101207  [30144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113652  [30784/57600]\n",
      "Accuracy: 98.4375, Loss: 0.052096  [31424/57600]\n",
      "Accuracy: 89.0625, Loss: 0.118798  [32064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086581  [32704/57600]\n",
      "Accuracy: 89.0625, Loss: 0.119043  [33344/57600]\n",
      "Accuracy: 87.5, Loss: 0.170763  [33984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.116768  [34624/57600]\n",
      "Accuracy: 89.0625, Loss: 0.173670  [35264/57600]\n",
      "Accuracy: 90.625, Loss: 0.104133  [35904/57600]\n",
      "Accuracy: 93.75, Loss: 0.085896  [36544/57600]\n",
      "Accuracy: 96.875, Loss: 0.049642  [37184/57600]\n",
      "Accuracy: 85.9375, Loss: 0.220562  [37824/57600]\n",
      "Accuracy: 87.5, Loss: 0.146032  [38464/57600]\n",
      "Accuracy: 90.625, Loss: 0.125652  [39104/57600]\n",
      "Accuracy: 87.5, Loss: 0.138672  [39744/57600]\n",
      "Accuracy: 90.625, Loss: 0.119016  [40384/57600]\n",
      "Accuracy: 96.875, Loss: 0.048550  [41024/57600]\n",
      "Accuracy: 96.875, Loss: 0.098048  [41664/57600]\n",
      "Accuracy: 87.5, Loss: 0.196353  [42304/57600]\n",
      "Accuracy: 85.9375, Loss: 0.197130  [42944/57600]\n",
      "Accuracy: 89.0625, Loss: 0.207101  [43584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.070668  [44224/57600]\n",
      "Accuracy: 96.875, Loss: 0.062361  [44864/57600]\n",
      "Accuracy: 82.8125, Loss: 0.207066  [45504/57600]\n",
      "Accuracy: 93.75, Loss: 0.084376  [46144/57600]\n",
      "Accuracy: 96.875, Loss: 0.078909  [46784/57600]\n",
      "Accuracy: 90.625, Loss: 0.118479  [47424/57600]\n",
      "Accuracy: 90.625, Loss: 0.116553  [48064/57600]\n",
      "Accuracy: 96.875, Loss: 0.055549  [48704/57600]\n",
      "Accuracy: 89.0625, Loss: 0.140914  [49344/57600]\n",
      "Accuracy: 95.3125, Loss: 0.068192  [49984/57600]\n",
      "Accuracy: 95.3125, Loss: 0.082233  [50624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.084100  [51264/57600]\n",
      "Accuracy: 93.75, Loss: 0.101279  [51904/57600]\n",
      "Accuracy: 93.75, Loss: 0.078012  [52544/57600]\n",
      "Accuracy: 93.75, Loss: 0.093274  [53184/57600]\n",
      "Accuracy: 89.0625, Loss: 0.132631  [53824/57600]\n",
      "Accuracy: 93.75, Loss: 0.115543  [54464/57600]\n",
      "Accuracy: 93.75, Loss: 0.087814  [55104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.064172  [55744/57600]\n",
      "Accuracy: 98.4375, Loss: 0.052653  [56384/57600]\n",
      "Accuracy: 82.8125, Loss: 0.196217  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.106227 \n",
      "           Precision: 0.885, Recall: 0.968\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Accuracy: 98.4375, Loss: 0.052178  [   64/57600]\n",
      "Accuracy: 89.0625, Loss: 0.123020  [  704/57600]\n",
      "Accuracy: 90.625, Loss: 0.112820  [ 1344/57600]\n",
      "Accuracy: 93.75, Loss: 0.077419  [ 1984/57600]\n",
      "Accuracy: 85.9375, Loss: 0.206611  [ 2624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.116042  [ 3264/57600]\n",
      "Accuracy: 96.875, Loss: 0.039014  [ 3904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.079423  [ 4544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.088755  [ 5184/57600]\n",
      "Accuracy: 93.75, Loss: 0.071154  [ 5824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.093600  [ 6464/57600]\n",
      "Accuracy: 95.3125, Loss: 0.103526  [ 7104/57600]\n",
      "Accuracy: 90.625, Loss: 0.090368  [ 7744/57600]\n",
      "Accuracy: 85.9375, Loss: 0.167020  [ 8384/57600]\n",
      "Accuracy: 93.75, Loss: 0.098750  [ 9024/57600]\n",
      "Accuracy: 98.4375, Loss: 0.043080  [ 9664/57600]\n",
      "Accuracy: 95.3125, Loss: 0.058189  [10304/57600]\n",
      "Accuracy: 93.75, Loss: 0.110046  [10944/57600]\n",
      "Accuracy: 90.625, Loss: 0.178442  [11584/57600]\n",
      "Accuracy: 89.0625, Loss: 0.112614  [12224/57600]\n",
      "Accuracy: 93.75, Loss: 0.105898  [12864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.130443  [13504/57600]\n",
      "Accuracy: 96.875, Loss: 0.093911  [14144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.080528  [14784/57600]\n",
      "Accuracy: 85.9375, Loss: 0.157770  [15424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.074132  [16064/57600]\n",
      "Accuracy: 93.75, Loss: 0.127730  [16704/57600]\n",
      "Accuracy: 87.5, Loss: 0.179942  [17344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.143027  [17984/57600]\n",
      "Accuracy: 93.75, Loss: 0.094021  [18624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.081017  [19264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.124702  [19904/57600]\n",
      "Accuracy: 96.875, Loss: 0.059236  [20544/57600]\n",
      "Accuracy: 87.5, Loss: 0.118311  [21184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100824  [21824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.095841  [22464/57600]\n",
      "Accuracy: 96.875, Loss: 0.062553  [23104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.069047  [23744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.102029  [24384/57600]\n",
      "Accuracy: 92.1875, Loss: 0.083200  [25024/57600]\n",
      "Accuracy: 95.3125, Loss: 0.071740  [25664/57600]\n",
      "Accuracy: 87.5, Loss: 0.122211  [26304/57600]\n",
      "Accuracy: 90.625, Loss: 0.092302  [26944/57600]\n",
      "Accuracy: 95.3125, Loss: 0.089416  [27584/57600]\n",
      "Accuracy: 95.3125, Loss: 0.064074  [28224/57600]\n",
      "Accuracy: 95.3125, Loss: 0.101380  [28864/57600]\n",
      "Accuracy: 92.1875, Loss: 0.123997  [29504/57600]\n",
      "Accuracy: 95.3125, Loss: 0.052112  [30144/57600]\n",
      "Accuracy: 98.4375, Loss: 0.046550  [30784/57600]\n",
      "Accuracy: 85.9375, Loss: 0.124931  [31424/57600]\n",
      "Accuracy: 95.3125, Loss: 0.094166  [32064/57600]\n",
      "Accuracy: 96.875, Loss: 0.055549  [32704/57600]\n",
      "Accuracy: 87.5, Loss: 0.189509  [33344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.090029  [33984/57600]\n",
      "Accuracy: 89.0625, Loss: 0.147390  [34624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.090072  [35264/57600]\n",
      "Accuracy: 85.9375, Loss: 0.132471  [35904/57600]\n",
      "Accuracy: 93.75, Loss: 0.095234  [36544/57600]\n",
      "Accuracy: 93.75, Loss: 0.063644  [37184/57600]\n",
      "Accuracy: 93.75, Loss: 0.072154  [37824/57600]\n",
      "Accuracy: 90.625, Loss: 0.089646  [38464/57600]\n",
      "Accuracy: 95.3125, Loss: 0.086977  [39104/57600]\n",
      "Accuracy: 87.5, Loss: 0.163606  [39744/57600]\n",
      "Accuracy: 89.0625, Loss: 0.107267  [40384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.060407  [41024/57600]\n",
      "Accuracy: 93.75, Loss: 0.072863  [41664/57600]\n",
      "Accuracy: 96.875, Loss: 0.049580  [42304/57600]\n",
      "Accuracy: 85.9375, Loss: 0.160173  [42944/57600]\n",
      "Accuracy: 93.75, Loss: 0.088093  [43584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.104142  [44224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.119672  [44864/57600]\n",
      "Accuracy: 90.625, Loss: 0.099770  [45504/57600]\n",
      "Accuracy: 90.625, Loss: 0.126137  [46144/57600]\n",
      "Accuracy: 90.625, Loss: 0.123236  [46784/57600]\n",
      "Accuracy: 90.625, Loss: 0.081758  [47424/57600]\n",
      "Accuracy: 82.8125, Loss: 0.176544  [48064/57600]\n",
      "Accuracy: 92.1875, Loss: 0.099257  [48704/57600]\n",
      "Accuracy: 92.1875, Loss: 0.088365  [49344/57600]\n",
      "Accuracy: 93.75, Loss: 0.086978  [49984/57600]\n",
      "Accuracy: 93.75, Loss: 0.093038  [50624/57600]\n",
      "Accuracy: 87.5, Loss: 0.171880  [51264/57600]\n",
      "Accuracy: 89.0625, Loss: 0.121457  [51904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.114237  [52544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100467  [53184/57600]\n",
      "Accuracy: 93.75, Loss: 0.083853  [53824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.112340  [54464/57600]\n",
      "Accuracy: 87.5, Loss: 0.134593  [55104/57600]\n",
      "Accuracy: 90.625, Loss: 0.131268  [55744/57600]\n",
      "Accuracy: 93.75, Loss: 0.090249  [56384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.153618  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.108801 \n",
      "           Precision: 0.891, Recall: 0.944\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Accuracy: 95.3125, Loss: 0.082182  [   64/57600]\n",
      "Accuracy: 89.0625, Loss: 0.121348  [  704/57600]\n",
      "Accuracy: 96.875, Loss: 0.056230  [ 1344/57600]\n",
      "Accuracy: 93.75, Loss: 0.072266  [ 1984/57600]\n",
      "Accuracy: 90.625, Loss: 0.140991  [ 2624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.070516  [ 3264/57600]\n",
      "Accuracy: 95.3125, Loss: 0.081323  [ 3904/57600]\n",
      "Accuracy: 98.4375, Loss: 0.043624  [ 4544/57600]\n",
      "Accuracy: 90.625, Loss: 0.130264  [ 5184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.098174  [ 5824/57600]\n",
      "Accuracy: 96.875, Loss: 0.047930  [ 6464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.083677  [ 7104/57600]\n",
      "Accuracy: 93.75, Loss: 0.097225  [ 7744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.059529  [ 8384/57600]\n",
      "Accuracy: 89.0625, Loss: 0.082799  [ 9024/57600]\n",
      "Accuracy: 96.875, Loss: 0.070415  [ 9664/57600]\n",
      "Accuracy: 90.625, Loss: 0.124259  [10304/57600]\n",
      "Accuracy: 93.75, Loss: 0.082463  [10944/57600]\n",
      "Accuracy: 92.1875, Loss: 0.099501  [11584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.086867  [12224/57600]\n",
      "Accuracy: 93.75, Loss: 0.092583  [12864/57600]\n",
      "Accuracy: 87.5, Loss: 0.159913  [13504/57600]\n",
      "Accuracy: 90.625, Loss: 0.104649  [14144/57600]\n",
      "Accuracy: 93.75, Loss: 0.071366  [14784/57600]\n",
      "Accuracy: 96.875, Loss: 0.046739  [15424/57600]\n",
      "Accuracy: 87.5, Loss: 0.129885  [16064/57600]\n",
      "Accuracy: 87.5, Loss: 0.148419  [16704/57600]\n",
      "Accuracy: 85.9375, Loss: 0.153975  [17344/57600]\n",
      "Accuracy: 87.5, Loss: 0.153397  [17984/57600]\n",
      "Accuracy: 95.3125, Loss: 0.087532  [18624/57600]\n",
      "Accuracy: 93.75, Loss: 0.069442  [19264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.126314  [19904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.110954  [20544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.079468  [21184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.103630  [21824/57600]\n",
      "Accuracy: 92.1875, Loss: 0.088950  [22464/57600]\n",
      "Accuracy: 85.9375, Loss: 0.186476  [23104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.095807  [23744/57600]\n",
      "Accuracy: 87.5, Loss: 0.112248  [24384/57600]\n",
      "Accuracy: 93.75, Loss: 0.085821  [25024/57600]\n",
      "Accuracy: 98.4375, Loss: 0.048645  [25664/57600]\n",
      "Accuracy: 95.3125, Loss: 0.069622  [26304/57600]\n",
      "Accuracy: 90.625, Loss: 0.123592  [26944/57600]\n",
      "Accuracy: 87.5, Loss: 0.138464  [27584/57600]\n",
      "Accuracy: 93.75, Loss: 0.110181  [28224/57600]\n",
      "Accuracy: 87.5, Loss: 0.154790  [28864/57600]\n",
      "Accuracy: 95.3125, Loss: 0.055976  [29504/57600]\n",
      "Accuracy: 95.3125, Loss: 0.058900  [30144/57600]\n",
      "Accuracy: 90.625, Loss: 0.084505  [30784/57600]\n",
      "Accuracy: 93.75, Loss: 0.115336  [31424/57600]\n",
      "Accuracy: 92.1875, Loss: 0.143029  [32064/57600]\n",
      "Accuracy: 84.375, Loss: 0.157420  [32704/57600]\n",
      "Accuracy: 85.9375, Loss: 0.151014  [33344/57600]\n",
      "Accuracy: 89.0625, Loss: 0.183025  [33984/57600]\n",
      "Accuracy: 95.3125, Loss: 0.082634  [34624/57600]\n",
      "Accuracy: 90.625, Loss: 0.140578  [35264/57600]\n",
      "Accuracy: 95.3125, Loss: 0.050592  [35904/57600]\n",
      "Accuracy: 92.1875, Loss: 0.132534  [36544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.080647  [37184/57600]\n",
      "Accuracy: 92.1875, Loss: 0.098663  [37824/57600]\n",
      "Accuracy: 90.625, Loss: 0.121425  [38464/57600]\n",
      "Accuracy: 89.0625, Loss: 0.166455  [39104/57600]\n",
      "Accuracy: 90.625, Loss: 0.106559  [39744/57600]\n",
      "Accuracy: 98.4375, Loss: 0.049911  [40384/57600]\n",
      "Accuracy: 87.5, Loss: 0.158592  [41024/57600]\n",
      "Accuracy: 98.4375, Loss: 0.039634  [41664/57600]\n",
      "Accuracy: 92.1875, Loss: 0.090823  [42304/57600]\n",
      "Accuracy: 90.625, Loss: 0.160063  [42944/57600]\n",
      "Accuracy: 92.1875, Loss: 0.091443  [43584/57600]\n",
      "Accuracy: 90.625, Loss: 0.084248  [44224/57600]\n",
      "Accuracy: 95.3125, Loss: 0.051652  [44864/57600]\n",
      "Accuracy: 90.625, Loss: 0.120087  [45504/57600]\n",
      "Accuracy: 93.75, Loss: 0.092462  [46144/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113061  [46784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.096529  [47424/57600]\n",
      "Accuracy: 98.4375, Loss: 0.057025  [48064/57600]\n",
      "Accuracy: 87.5, Loss: 0.138621  [48704/57600]\n",
      "Accuracy: 95.3125, Loss: 0.054298  [49344/57600]\n",
      "Accuracy: 90.625, Loss: 0.110018  [49984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.078919  [50624/57600]\n",
      "Accuracy: 93.75, Loss: 0.076691  [51264/57600]\n",
      "Accuracy: 90.625, Loss: 0.141927  [51904/57600]\n",
      "Accuracy: 95.3125, Loss: 0.085200  [52544/57600]\n",
      "Accuracy: 96.875, Loss: 0.052839  [53184/57600]\n",
      "Accuracy: 87.5, Loss: 0.135123  [53824/57600]\n",
      "Accuracy: 87.5, Loss: 0.150131  [54464/57600]\n",
      "Accuracy: 90.625, Loss: 0.110157  [55104/57600]\n",
      "Accuracy: 92.1875, Loss: 0.081218  [55744/57600]\n",
      "Accuracy: 90.625, Loss: 0.093989  [56384/57600]\n",
      "Accuracy: 95.3125, Loss: 0.065237  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.105312 \n",
      "           Precision: 0.889, Recall: 0.962\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Accuracy: 95.3125, Loss: 0.088248  [   64/57600]\n",
      "Accuracy: 95.3125, Loss: 0.073230  [  704/57600]\n",
      "Accuracy: 95.3125, Loss: 0.071611  [ 1344/57600]\n",
      "Accuracy: 93.75, Loss: 0.097195  [ 1984/57600]\n",
      "Accuracy: 98.4375, Loss: 0.044857  [ 2624/57600]\n",
      "Accuracy: 89.0625, Loss: 0.157541  [ 3264/57600]\n",
      "Accuracy: 87.5, Loss: 0.169523  [ 3904/57600]\n",
      "Accuracy: 93.75, Loss: 0.078971  [ 4544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.080085  [ 5184/57600]\n",
      "Accuracy: 95.3125, Loss: 0.105299  [ 5824/57600]\n",
      "Accuracy: 93.75, Loss: 0.076343  [ 6464/57600]\n",
      "Accuracy: 93.75, Loss: 0.069290  [ 7104/57600]\n",
      "Accuracy: 87.5, Loss: 0.120605  [ 7744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.066172  [ 8384/57600]\n",
      "Accuracy: 93.75, Loss: 0.067739  [ 9024/57600]\n",
      "Accuracy: 95.3125, Loss: 0.070823  [ 9664/57600]\n",
      "Accuracy: 98.4375, Loss: 0.046642  [10304/57600]\n",
      "Accuracy: 87.5, Loss: 0.110955  [10944/57600]\n",
      "Accuracy: 85.9375, Loss: 0.128926  [11584/57600]\n",
      "Accuracy: 89.0625, Loss: 0.117549  [12224/57600]\n",
      "Accuracy: 89.0625, Loss: 0.153105  [12864/57600]\n",
      "Accuracy: 93.75, Loss: 0.106817  [13504/57600]\n",
      "Accuracy: 92.1875, Loss: 0.076922  [14144/57600]\n",
      "Accuracy: 87.5, Loss: 0.198764  [14784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.134730  [15424/57600]\n",
      "Accuracy: 93.75, Loss: 0.089203  [16064/57600]\n",
      "Accuracy: 95.3125, Loss: 0.055362  [16704/57600]\n",
      "Accuracy: 90.625, Loss: 0.165964  [17344/57600]\n",
      "Accuracy: 93.75, Loss: 0.086362  [17984/57600]\n",
      "Accuracy: 87.5, Loss: 0.146119  [18624/57600]\n",
      "Accuracy: 90.625, Loss: 0.085192  [19264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.120845  [19904/57600]\n",
      "Accuracy: 95.3125, Loss: 0.065481  [20544/57600]\n",
      "Accuracy: 93.75, Loss: 0.087774  [21184/57600]\n",
      "Accuracy: 90.625, Loss: 0.114745  [21824/57600]\n",
      "Accuracy: 90.625, Loss: 0.100604  [22464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.117718  [23104/57600]\n",
      "Accuracy: 95.3125, Loss: 0.076926  [23744/57600]\n",
      "Accuracy: 95.3125, Loss: 0.066105  [24384/57600]\n",
      "Accuracy: 90.625, Loss: 0.122954  [25024/57600]\n",
      "Accuracy: 96.875, Loss: 0.060429  [25664/57600]\n",
      "Accuracy: 93.75, Loss: 0.110515  [26304/57600]\n",
      "Accuracy: 87.5, Loss: 0.131445  [26944/57600]\n",
      "Accuracy: 90.625, Loss: 0.121805  [27584/57600]\n",
      "Accuracy: 93.75, Loss: 0.086925  [28224/57600]\n",
      "Accuracy: 92.1875, Loss: 0.092001  [28864/57600]\n",
      "Accuracy: 89.0625, Loss: 0.097668  [29504/57600]\n",
      "Accuracy: 90.625, Loss: 0.105792  [30144/57600]\n",
      "Accuracy: 89.0625, Loss: 0.136307  [30784/57600]\n",
      "Accuracy: 92.1875, Loss: 0.108862  [31424/57600]\n",
      "Accuracy: 90.625, Loss: 0.066560  [32064/57600]\n",
      "Accuracy: 93.75, Loss: 0.109633  [32704/57600]\n",
      "Accuracy: 93.75, Loss: 0.068528  [33344/57600]\n",
      "Accuracy: 93.75, Loss: 0.091751  [33984/57600]\n",
      "Accuracy: 82.8125, Loss: 0.234020  [34624/57600]\n",
      "Accuracy: 95.3125, Loss: 0.075912  [35264/57600]\n",
      "Accuracy: 90.625, Loss: 0.123817  [35904/57600]\n",
      "Accuracy: 96.875, Loss: 0.047552  [36544/57600]\n",
      "Accuracy: 92.1875, Loss: 0.081289  [37184/57600]\n",
      "Accuracy: 93.75, Loss: 0.083084  [37824/57600]\n",
      "Accuracy: 95.3125, Loss: 0.091745  [38464/57600]\n",
      "Accuracy: 93.75, Loss: 0.090918  [39104/57600]\n",
      "Accuracy: 93.75, Loss: 0.070978  [39744/57600]\n",
      "Accuracy: 98.4375, Loss: 0.043141  [40384/57600]\n",
      "Accuracy: 98.4375, Loss: 0.041593  [41024/57600]\n",
      "Accuracy: 90.625, Loss: 0.128533  [41664/57600]\n",
      "Accuracy: 90.625, Loss: 0.138261  [42304/57600]\n",
      "Accuracy: 92.1875, Loss: 0.066633  [42944/57600]\n",
      "Accuracy: 92.1875, Loss: 0.128596  [43584/57600]\n",
      "Accuracy: 92.1875, Loss: 0.101737  [44224/57600]\n",
      "Accuracy: 93.75, Loss: 0.067076  [44864/57600]\n",
      "Accuracy: 93.75, Loss: 0.093015  [45504/57600]\n",
      "Accuracy: 95.3125, Loss: 0.078533  [46144/57600]\n",
      "Accuracy: 98.4375, Loss: 0.041589  [46784/57600]\n",
      "Accuracy: 90.625, Loss: 0.119572  [47424/57600]\n",
      "Accuracy: 95.3125, Loss: 0.083160  [48064/57600]\n",
      "Accuracy: 98.4375, Loss: 0.056164  [48704/57600]\n",
      "Accuracy: 96.875, Loss: 0.070717  [49344/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100296  [49984/57600]\n",
      "Accuracy: 89.0625, Loss: 0.185164  [50624/57600]\n",
      "Accuracy: 90.625, Loss: 0.121105  [51264/57600]\n",
      "Accuracy: 92.1875, Loss: 0.113171  [51904/57600]\n",
      "Accuracy: 85.9375, Loss: 0.147768  [52544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.075161  [53184/57600]\n",
      "Accuracy: 98.4375, Loss: 0.056179  [53824/57600]\n",
      "Accuracy: 85.9375, Loss: 0.130331  [54464/57600]\n",
      "Accuracy: 92.1875, Loss: 0.124451  [55104/57600]\n",
      "Accuracy: 98.4375, Loss: 0.059067  [55744/57600]\n",
      "Accuracy: 90.625, Loss: 0.103685  [56384/57600]\n",
      "Accuracy: 92.1875, Loss: 0.073244  [57024/57600]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.106969 \n",
      "           Precision: 0.883, Recall: 0.964\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Accuracy: 93.75, Loss: 0.071460  [   64/57600]\n",
      "Accuracy: 93.75, Loss: 0.117197  [  704/57600]\n",
      "Accuracy: 96.875, Loss: 0.050400  [ 1344/57600]\n",
      "Accuracy: 90.625, Loss: 0.122292  [ 1984/57600]\n",
      "Accuracy: 92.1875, Loss: 0.132904  [ 2624/57600]\n",
      "Accuracy: 92.1875, Loss: 0.100416  [ 3264/57600]\n",
      "Accuracy: 95.3125, Loss: 0.079589  [ 3904/57600]\n",
      "Accuracy: 89.0625, Loss: 0.158730  [ 4544/57600]\n",
      "Accuracy: 95.3125, Loss: 0.073038  [ 5184/57600]\n",
      "Accuracy: 93.75, Loss: 0.088691  [ 5824/57600]\n",
      "Accuracy: 93.75, Loss: 0.087252  [ 6464/57600]\n",
      "Accuracy: 96.875, Loss: 0.050549  [ 7104/57600]\n",
      "Accuracy: 90.625, Loss: 0.081353  [ 7744/57600]\n",
      "Accuracy: 92.1875, Loss: 0.079361  [ 8384/57600]\n",
      "Accuracy: 93.75, Loss: 0.102712  [ 9024/57600]\n",
      "Accuracy: 93.75, Loss: 0.079242  [ 9664/57600]\n",
      "Accuracy: 95.3125, Loss: 0.062735  [10304/57600]\n",
      "Accuracy: 93.75, Loss: 0.062963  [10944/57600]\n",
      "Accuracy: 93.75, Loss: 0.076998  [11584/57600]\n",
      "Accuracy: 93.75, Loss: 0.076673  [12224/57600]\n",
      "Accuracy: 87.5, Loss: 0.149138  [12864/57600]\n",
      "Accuracy: 81.25, Loss: 0.229781  [13504/57600]\n",
      "Accuracy: 89.0625, Loss: 0.141145  [14144/57600]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     loop_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     test_loop(test_dataloader, classifier, binary_loss)\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(classifier\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_classifier_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_checkpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     37\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 39\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mStateData.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[1;32m     12\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/serialization.py:1090\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1090\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1096\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[1;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/_weights_only_unpickler.py:342\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     ):\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    340\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m         )\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    344\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/tractography/lib/python3.12/site-packages/torch/serialization.py:1460\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbyteorderdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m sys\u001b[38;5;241m.\u001b[39mbyteorder:\n\u001b[1;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_dir = \"/home/brysongray/tractography/pretrained_models/neurom_no_artifacts_classifier/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "lr = 1e-3\n",
    "epochs = 25\n",
    "in_channels = 4\n",
    "\n",
    "# classifier = Net(chin=in_channels, chout=3)\n",
    "classifier = ResNet(ResidualBlock, [3, 4, 6, 3], num_classes=1)\n",
    "classifier = classifier.to(device=DEVICE, dtype=dtype)\n",
    "\n",
    "#load a previously trained model\n",
    "# classifier_state_dict = torch.load(\"/home/brysongray/tractography/pretrained_models/neurom_with_artifacts_classifier/resnet_classifier_01-24-25_checkpoint-6.pt\", weights_only=True)\n",
    "# classifier.load_state_dict(classifier_state_dict)\n",
    "\n",
    "classifier.train()\n",
    "classifier_optimizer = optim.AdamW(classifier.parameters(), lr=lr)\n",
    "binary_loss = torch.nn.BCELoss()\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "for i,t in enumerate(range(epochs)):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loop_losses = train_loop(train_dataloader, classifier, binary_loss, classifier_optimizer)\n",
    "    test_loop(test_dataloader, classifier, binary_loss)\n",
    "    torch.save(classifier.state_dict(), os.path.join(out_dir, f\"resnet_classifier_{date}_checkpoint-{i}.pt\"))\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.092713 \n",
      "           Precision: 0.905, Recall: 0.967\n"
     ]
    }
   ],
   "source": [
    "classifier = ResNet(ResidualBlock, [3, 4, 6, 3], num_classes=1)\n",
    "classifier = classifier.to(device=DEVICE, dtype=dtype)\n",
    "classifier_state_dict = torch.load(\"/home/brysongray/tractography/pretrained_models/resnet_classifier_01-07-25_checkpoint-42.pt\", weights_only=True)\n",
    "classifier.load_state_dict(classifier_state_dict)\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, sampler=test_sampler)\n",
    "binary_loss = torch.nn.BCELoss()\n",
    "\n",
    "test_loop(test_dataloader, classifier, binary_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.105862 \n",
      "           Precision: 0.912, Recall: 0.942\n"
     ]
    }
   ],
   "source": [
    "classifier = ResNet(ResidualBlock, [3, 4, 6, 3], num_classes=1)\n",
    "classifier = classifier.to(device=DEVICE, dtype=dtype)\n",
    "classifier_state_dict = torch.load(\"/home/brysongray/tractography/pretrained_models/neurom_with_artifacts_classifier/resnet_classifier_01-24-25_checkpoint-16.pt\", weights_only=True)\n",
    "classifier.load_state_dict(classifier_state_dict)\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, sampler=test_sampler)\n",
    "binary_loss = torch.nn.BCELoss()\n",
    "\n",
    "test_loop(test_dataloader, classifier, binary_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.100358 \n",
      "           Precision: 0.889, Recall: 0.972\n"
     ]
    }
   ],
   "source": [
    "classifier = ResNet(ResidualBlock, [3, 4, 6, 3], num_classes=1)\n",
    "classifier = classifier.to(device=DEVICE, dtype=dtype)\n",
    "classifier_state_dict = torch.load(\"/home/brysongray/tractography/pretrained_models/neurom_no_artifacts_classifier/resnet_classifier_01-25-25_checkpoint-12.pt\", weights_only=True)\n",
    "classifier.load_state_dict(classifier_state_dict)\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, sampler=test_sampler)\n",
    "binary_loss = torch.nn.BCELoss()\n",
    "\n",
    "test_loop(test_dataloader, classifier, binary_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926757281553398\n",
      "0.9286491133799033\n"
     ]
    }
   ],
   "source": [
    "def fscore(precision,recall):\n",
    "    return 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(fscore(0.912,0.942))\n",
    "print(fscore(0.889,0.972))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iter(test_dataloader))\n",
    "\n",
    "fig, ax = plt.subplots(5,5, figsize=(10,10))\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i,x in enumerate(ax):\n",
    "\n",
    "    img = X[i, :3].amax(1).permute(1,2,0).cpu()\n",
    "    # path = X[i, 3].amax(1).cpu()\n",
    "    x.imshow(img)\n",
    "    # x.imshow(path, alpha=0.5)\n",
    "    x.axis(\"off\")\n",
    "    pred = classifier(X[i][None].to(device=DEVICE))\n",
    "    pred = torch.nn.functional.sigmoid(pred.squeeze())\n",
    "    # pred = torch.argmax(pred).cpu()\n",
    "    x.set_title(f\"label: {y[i]}, pred: {pred.item():.2f}\", fontsize=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tractography",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
